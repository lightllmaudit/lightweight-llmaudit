{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3f781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 05:02:21 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 05:02:21 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 05:02:23,729\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [03:23<00:00, 29.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-34b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97abb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'smart_contract_train.csv',\n",
    "    'val': 'smart_contract_val.csv',\n",
    "    'test': 'smart_contract_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world that **only** answer/respond in term of \"Vulnerable Code\" or \"Safe Code\" \"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given function and classify it as \"Vulnerable Code\" or \"Safe Code\" based on potential security risks. Only answer the label dont give any explanation.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\"\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, prompt):\n",
    "    code = examples['code']\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    vulnerable = \"Vulnerable Code\" if examples['vulnerable'] == 1 else \"Safe Code\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": selected_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(code=code)}\n",
    "    ]\n",
    "    return {\"conversations\": conversation, \"label\": vulnerable, \"vuln_code\": code}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i, prompt in enumerate(PROMPTS):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, prompt))\n",
    "    new_dataset = new_dataset.remove_columns([\"project_id\", \"code\", \"code_analysis\", \"vulnerable\"])\n",
    "    datasets.append(new_dataset)\n",
    "    \n",
    "test_dataset = datasets[0]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb84077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/278 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [11:05<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 85.09%\n",
      "Std GPU util: 9.99%\n",
      "Peak GPU mem: 19.5986 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                             temperature = 0.6, min_p = 0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "    \n",
    "    # Extract the label using regex\n",
    "    match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "    extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "    # Append the extracted label\n",
    "    y_pred.append(extracted_label)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad50bd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 19.207 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66342e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 05:29:16 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 05:29:17 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 05:29:18,957\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:57<00:00, 19.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-13b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3754255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'smart_contract_train.csv',\n",
    "    'val': 'smart_contract_val.csv',\n",
    "    'test': 'smart_contract_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world that **only** answer/respond in term of \"Vulnerable Code\" or \"Safe Code\" \"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given function and classify it as \"Vulnerable Code\" or \"Safe Code\" based on potential security risks. Only answer the label dont give any explanation.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\"\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, prompt):\n",
    "    code = examples['code']\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    vulnerable = \"Vulnerable Code\" if examples['vulnerable'] == 1 else \"Safe Code\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": selected_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(code=code)}\n",
    "    ]\n",
    "    return {\"conversations\": conversation, \"label\": vulnerable, \"vuln_code\": code}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i, prompt in enumerate(PROMPTS):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, prompt))\n",
    "    new_dataset = new_dataset.remove_columns([\"project_id\", \"code\", \"code_analysis\", \"vulnerable\"])\n",
    "    datasets.append(new_dataset)\n",
    "    \n",
    "test_dataset = datasets[0]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccf2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/278 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [03:08<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 66.42%\n",
      "Std GPU util: 22.95%\n",
      "Peak GPU mem: 11.0928 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                             temperature = 0.6, min_p = 0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "    \n",
    "    # Extract the label using regex\n",
    "    match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "    extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "    # Append the extracted label\n",
    "    y_pred.append(extracted_label)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e836fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.332 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90883431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 05:34:04 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 05:34:04 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 05:34:06,728\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:39<00:00, 13.15s/it]\n",
      "Unsloth 2025.5.3 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama_detector\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb75614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'smart_contract_train.csv',\n",
    "    'val': 'smart_contract_val.csv',\n",
    "    'test': 'smart_contract_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf093d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world.\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given function and classify it as \"Vulnerable Code\" or \"Safe Code\" based on potential security risks.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['code']\n",
    "    vulnerable = \"Vulnerable Code\" if examples['vulnerable'] == 1 else \"Safe Code\"\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code)},\n",
    "                    {\"role\": \"assistant\", \"content\": vulnerable}]\n",
    "    return { \"conversations\" : conversation, }\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "dataset = dataset.remove_columns([\"project_id\",\"code\", \"code_analysis\", \"vulnerable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1ae4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026aa828",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']\n",
    "test_dataset = test_dataset.map(lambda row: {'label':row['conversations'][-1]['content']})\n",
    "test_dataset = test_dataset.map(lambda row: {'conversations':row['conversations'][:-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989374e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s> [INST] <<SYS>>\\nYou are the smartest AI solidity smart contract security auditor in the world.\\n<</SYS>>\\n\\nYou need to analyze the given function and classify it as \"Vulnerable Code\" or \"Safe Code\" based on potential security risks.\\n    \\n    This is the function we need to audit:\\n    ```solidity\\n    function _getAdmin(uint256 adminEpoch, uint256 index) internal view returns (address) {\\n        return getAddress(_getAdminKey(adminEpoch, index));\\n    }\\n\\nfunction _isAdmin(uint256 adminEpoch, address account) internal view returns (bool) {\\n        return getBool(_getIsAdminKey(adminEpoch, account));\\n    }\\n\\nfunction _setAdminEpoch(uint256 adminEpoch) internal {\\n        _setUint(KEY_ADMIN_EPOCH, adminEpoch);\\n    }\\n    ```\\n    \\nAnswer:  [/INST] Safe Code </s>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = test_dataset['conversations'][1]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d292294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [03:15<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 62.41%\n",
      "Std GPU util: 24.79%\n",
      "Peak GPU mem: 12.1436 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                             temperature = 0.6, min_p = 0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "    \n",
    "    # Extract the label using regex\n",
    "    match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "    extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "    # Append the extracted label\n",
    "    y_pred.append(extracted_label)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a0586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.332 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c221326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 05:40:59 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 05:40:59 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 05:41:01,823\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 32768\n",
    "dtype = None \n",
    "load_in_4bit = False \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ours_detector\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    full_finetuning = True\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'smart_contract_train.csv',\n",
    "    'val': 'smart_contract_val.csv',\n",
    "    'test': 'smart_contract_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world.\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant smart contract security auditor in the world.\"\"\",\n",
    "    \"\"\"You are the best solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant solidity security researcher in the world\"\"\",\n",
    "    \"\"\"You are the best AI solidity smart contract security auditor in the world.\"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given function and classify it as \"Vulnerable Code\" or \"Safe Code\" based on potential security risks.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\",\n",
    "    \n",
    "    \"\"\"Analyze the given function and determine whether it is \"Vulnerable Code\" or \"Safe Code\" based on security risks.\n",
    "    \n",
    "    Function to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\",\n",
    "\n",
    "    \"\"\"Examine the Solidity function below and assess if it is \"Vulnerable Code\" or \"Safe Code.\"\n",
    "    \n",
    "    Solidity Function:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\",\n",
    "\n",
    "    \"\"\"Review the Solidity function and classify it as \"Vulnerable Code\" or \"Safe Code\" by checking for security issues.\n",
    "    \n",
    "    Solidity Function:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\",\n",
    "\n",
    "    \"\"\"Audit the given Solidity function to determine if it should be categorized as \"Vulnerable Code\" or \"Safe Code.\"\n",
    "    \n",
    "    Solidity Code:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Answer: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    code = examples['code']\n",
    "    vulnerable = \"Vulnerable Code\" if examples['vulnerable'] == 1 else \"Safe Code\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(code=code)}\n",
    "    ]\n",
    "    return {\"conversations\": conversation, \"label\": vulnerable}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"project_id\", \"code\", \"code_analysis\", \"vulnerable\"])\n",
    "    datasets.append(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598c8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [05:02<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 26.65%\n",
      "Std GPU util: 3.81%\n",
      "Peak GPU mem: 3.7881 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Regex pattern for extracting labels\n",
    "pattern = r\"<\\|im_start\\|>assistant\\n<think>\\n\\n</think>\\n\\n(.*?)<\\|im_end\\|>\"\n",
    "\n",
    "y_preds = [[] for _ in range(5)]\n",
    "row_times = []  # Store total time per row (inference + voting)\n",
    "final_predictions = []\n",
    "\n",
    "# Iterate through rows with normal tqdm\n",
    "for row_idx, messages_set in enumerate(tqdm(zip(*[d['test']['conversations'] for d in datasets]), desc=\"Processing rows\", total=dataset['test'].num_rows)):\n",
    "    total_start = time.time()  # Start timer for this row\n",
    "\n",
    "    preds_for_row = []\n",
    "\n",
    "    # Inference for each dataset\n",
    "    for i, messages in enumerate(messages_set):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            enable_thinking=False,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=8,\n",
    "            use_cache=True,\n",
    "            temperature=0.1,\n",
    "            min_p=0.1\n",
    "        )\n",
    "        decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "        match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "        extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "        y_preds[i].append(extracted_label)\n",
    "        preds_for_row.append(extracted_label)\n",
    "\n",
    "    # Majority voting\n",
    "    vote_count = {\"Vulnerable Code\": 0, \"Safe Code\": 0}\n",
    "    for pred in preds_for_row:\n",
    "        if pred in vote_count:\n",
    "            vote_count[pred] += 1\n",
    "    final_prediction = max(vote_count, key=vote_count.get)\n",
    "    final_predictions.append(final_prediction)\n",
    "\n",
    "    total_end = time.time()  # Stop timer for this row\n",
    "    row_times.append(total_end - total_start)\n",
    "\n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3c9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 3.398 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/gpu_utils_ours_detector.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_ours_detector.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f9d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:03<00:00, 71.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 43.79%\n",
      "Std GPU util: 24.55%\n",
      "Peak GPU mem: 0.9111 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./codebert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "data_files = {'test': 'smart_contract_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "datasets = datasets.rename_column('vulnerable', 'labels')\n",
    "datasets = datasets.map(lambda example: {'labels': int(example['labels'])})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "label_map = {0: \"Not Vulnerable\", 1: \"Vulnerable\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "        \n",
    "        code_snippet = example['code']  # Adjust column name if needed\n",
    "        inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        predictions.append({\n",
    "            \"code\": code_snippet,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "        \n",
    "        row_end = time.time()\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7577cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 0.543 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8307c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_codebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_codebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4b507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:03<00:00, 71.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 43.53%\n",
      "Std GPU util: 24.85%\n",
      "Peak GPU mem: 0.9111 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./graphcodebert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "data_files = {'test': 'smart_contract_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "datasets = datasets.rename_column('vulnerable', 'labels')\n",
    "datasets = datasets.map(lambda example: {'labels': int(example['labels'])})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "label_map = {0: \"Not Vulnerable\", 1: \"Vulnerable\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "        \n",
    "        code_snippet = example['code']  # Adjust column name if needed\n",
    "        inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        predictions.append({\n",
    "            \"code\": code_snippet,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "        \n",
    "        row_end = time.time()\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff50fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 0.543 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61427db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_graphcodebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_graphcodebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581db280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:03<00:00, 72.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 33.95%\n",
      "Std GPU util: 21.73%\n",
      "Peak GPU mem: 0.9150 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./unixcoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "data_files = {'test': 'smart_contract_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "datasets = datasets.rename_column('vulnerable', 'labels')\n",
    "datasets = datasets.map(lambda example: {'labels': int(example['labels'])})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "label_map = {0: \"Not Vulnerable\", 1: \"Vulnerable\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "        \n",
    "        code_snippet = example['code']  # Adjust column name if needed\n",
    "        inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        predictions.append({\n",
    "            \"code\": code_snippet,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "        \n",
    "        row_end = time.time()\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a453fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 0.547 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_unixcoder.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_unixcoder.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be700d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/278 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:11<00:00, 25.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 39.60%\n",
      "Std GPU util: 21.00%\n",
      "Peak GPU mem: 1.5576 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./codet5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "data_files = {'test': 'smart_contract_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "datasets = datasets.rename_column('vulnerable', 'labels')\n",
    "datasets = datasets.map(lambda example: {'labels': int(example['labels'])})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "label_map = {0: \"Not Vulnerable\", 1: \"Vulnerable\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "        \n",
    "        code_snippet = example['code']  # Adjust column name if needed\n",
    "        inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        predictions.append({\n",
    "            \"code\": code_snippet,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "        \n",
    "        row_end = time.time()\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1d5472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 1.189 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/gpu_utils_codet5.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_codet5.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05458c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_gpu_util</th>\n",
       "      <th>std_gpu_util</th>\n",
       "      <th>max_gpu_mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codebert</td>\n",
       "      <td>43.79</td>\n",
       "      <td>24.55</td>\n",
       "      <td>0.9111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>codet5</td>\n",
       "      <td>39.60</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1.5576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ftcodellama13b</td>\n",
       "      <td>62.41</td>\n",
       "      <td>24.79</td>\n",
       "      <td>12.1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graphcodebert</td>\n",
       "      <td>43.53</td>\n",
       "      <td>24.85</td>\n",
       "      <td>0.9111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ours_detector</td>\n",
       "      <td>26.65</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.7881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unixcoder</td>\n",
       "      <td>33.95</td>\n",
       "      <td>21.73</td>\n",
       "      <td>0.9150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zs_codellama13b</td>\n",
       "      <td>66.42</td>\n",
       "      <td>22.95</td>\n",
       "      <td>11.0928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zs_codellama34b</td>\n",
       "      <td>85.09</td>\n",
       "      <td>9.99</td>\n",
       "      <td>19.5986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  avg_gpu_util  std_gpu_util  max_gpu_mem\n",
       "0         codebert         43.79         24.55       0.9111\n",
       "1           codet5         39.60         21.00       1.5576\n",
       "2   ftcodellama13b         62.41         24.79      12.1436\n",
       "3    graphcodebert         43.53         24.85       0.9111\n",
       "4    ours_detector         26.65          3.81       3.7881\n",
       "5        unixcoder         33.95         21.73       0.9150\n",
       "6  zs_codellama13b         66.42         22.95      11.0928\n",
       "7  zs_codellama34b         85.09          9.99      19.5986"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = [\n",
    "    \"zs_codellama34b\",\n",
    "    \"zs_codellama13b\",\n",
    "    \"ftcodellama13b\",\n",
    "    \"ours_detector\",\n",
    "    \"codebert\",\n",
    "    \"graphcodebert\",\n",
    "    \"unixcoder\",\n",
    "    \"codet5\"\n",
    "]\n",
    "\n",
    "data_dir = \"_resource_data\"\n",
    "summary_data = {\n",
    "    \"model\": [],\n",
    "    \"avg_gpu_util\": [],\n",
    "    \"std_gpu_util\": [],\n",
    "    \"max_gpu_mem\": []\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    with open(f\"{data_dir}/gpu_utils_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_utils = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{data_dir}/gpu_mem_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_mem = pickle.load(f)\n",
    "    \n",
    "    avg_gpu_util = np.mean(gpu_utils)\n",
    "    std_gpu_util = np.std(gpu_utils, ddof=1)\n",
    "    max_gpu_mem = np.max(gpu_mem)\n",
    "    \n",
    "    summary_data[\"model\"].append(model)\n",
    "    summary_data[\"avg_gpu_util\"].append(float(f\"{avg_gpu_util:.2f}\"))\n",
    "    summary_data[\"std_gpu_util\"].append(float(f\"{std_gpu_util:.2f}\"))\n",
    "    summary_data[\"max_gpu_mem\"].append(float(f\"{max_gpu_mem:.4f}\"))\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.sort_values(by=\"model\", inplace=True)\n",
    "df_summary.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3889c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 06:01:00 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 06:01:00 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:01:02,506\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [03:20<00:00, 28.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-34b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': 'severity_data_train.csv',\n",
    "    'val': 'severity_data_val.csv',\n",
    "    'test': 'severity_data_test.csv'\n",
    "}\n",
    "\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world that only answer in one word between \"low\", \"medium\" or \"high\".\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given vulnerability explanation and classify the severity of it as \"low\", \"medium\" or \"high\" based on the impact of that vulnerability. Dont give any additional explanations just give the label (low, medium or high).\n",
    "    \n",
    "    This is the vulnerability explanation we need to analyze:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    explanation = examples['vuln_explanation']\n",
    "    severity = examples['severity']\n",
    "    selected_system_prompt = system_prompt\n",
    "    selected_user_prompt = prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(explanation=explanation)}]\n",
    "    \n",
    "    return { \"conversations\" : conversation, 'label': severity}\n",
    "\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"file_name\", \"vuln_title\", \"vuln_recommendation\", \"vuln_code\"])\n",
    "    datasets.append(new_dataset)\n",
    "    \n",
    "test_dataset = datasets[0]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5106c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [02:12<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 90.22%\n",
      "Std GPU util: 17.20%\n",
      "Peak GPU mem: 19.5791 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                             temperature = 0.6, min_p = 0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "    \n",
    "    # Extract the label using regex\n",
    "    match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "    extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "    # Append the extracted label\n",
    "    y_pred.append(extracted_label)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1507998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 19.188 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b333bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6624b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 06:11:20 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 06:11:20 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:11:22,730\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:16<00:00, 25.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-13b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadad47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': 'severity_data_train.csv',\n",
    "    'val': 'severity_data_val.csv',\n",
    "    'test': 'severity_data_test.csv'\n",
    "}\n",
    "\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world that only answer in one word between \"low\", \"medium\" or \"high\".\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given vulnerability explanation and classify the severity of it as \"low\", \"medium\" or \"high\" based on the impact of that vulnerability. Dont give any additional explanations just give the label (low, medium or high).\n",
    "    \n",
    "    This is the vulnerability explanation we need to analyze:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    explanation = examples['vuln_explanation']\n",
    "    severity = examples['severity']\n",
    "    selected_system_prompt = system_prompt\n",
    "    selected_user_prompt = prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(explanation=explanation)}]\n",
    "    \n",
    "    return { \"conversations\" : conversation, 'label': severity}\n",
    "\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"file_name\", \"vuln_title\", \"vuln_recommendation\", \"vuln_code\"])\n",
    "    datasets.append(new_dataset)\n",
    "    \n",
    "test_dataset = datasets[0]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac9c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:58<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 77.57%\n",
      "Std GPU util: 23.04%\n",
      "Peak GPU mem: 10.2900 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                             temperature = 0.6, min_p = 0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "    \n",
    "    # Extract the label using regex\n",
    "    match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "    extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "    # Append the extracted label\n",
    "    y_pred.append(extracted_label)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ed62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.332 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9de0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 08:46:30 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 08:46:30 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 08:46:32,841\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:25<00:00,  8.53s/it]\n",
      "Unsloth 2025.5.3 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"severity_codellama\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'severity_data_train.csv',\n",
    "    'val': 'severity_data_val.csv',\n",
    "    'test': 'severity_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world that only answer in one word between \"low\", \"medium\" or \"high\".\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given vulnerability explanation and classify the severity of it as \"low\", \"medium\" or \"high\" based on the impact of that vulnerability. Dont give any additional explanations just give the label (low, medium or high).\n",
    "    \n",
    "    This is the vulnerability explanation we need to analyze:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    explanation = examples['vuln_explanation']\n",
    "    severity = examples['severity']\n",
    "    selected_system_prompt = system_prompt\n",
    "    selected_user_prompt = prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(explanation=explanation)}]\n",
    "    \n",
    "    return { \"conversations\" : conversation, 'label': severity}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"file_name\", \"vuln_title\", \"vuln_explanation\", \"vuln_recommendation\", \"vuln_code\"])\n",
    "    datasets.append(new_dataset)\n",
    "    \n",
    "test_dataset = datasets[0]['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cd7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                             temperature = 0.6, min_p = 0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "    \n",
    "    # Extract the label using regex\n",
    "    match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "    extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "\n",
    "    # Append the extracted label\n",
    "    y_pred.append(extracted_label)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9698c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76f9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 06:18:12 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 06:18:12 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:18:14,784\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 32768\n",
    "dtype = None \n",
    "load_in_4bit = False \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ours_severity\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    full_finetuning = True\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e7607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 5176.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4839.41 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4799.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6137.45 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4887.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4840.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6171.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4986.40 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4948.44 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6220.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 5017.15 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4884.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6236.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 5000.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4942.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'severity_data_train.csv',\n",
    "    'val': 'severity_data_val.csv',\n",
    "    'test': 'severity_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world.\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant smart contract security auditor in the world.\"\"\",\n",
    "    \"\"\"You are the best solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant solidity security researcher in the world\"\"\",\n",
    "    \"\"\"You are the best AI solidity smart contract security auditor in the world.\"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"You need to analyze the given vulnerability explanation and classify the severity of it as \"low\", \"medium\" or \"high\" based on the impact of that vulnerability.\n",
    "    \n",
    "    This is the vulnerability explanation we need to analyze:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "    \n",
    "    \"\"\"Analyze the given vulnerability explanation and determine whether the severity is \"low\", \"medium\" or \"high\" based on the impact if the vulnerability exploited.\n",
    "    \n",
    "    Vulnerability explanation to analyze:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "\n",
    "    \"\"\"Examine the given vulnerability explanation below and assess the severity is it \"low\", \"medium\" or \"high\".\n",
    "    \n",
    "    Vulnerability explanation to examine:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "\n",
    "    \"\"\"Review the vulnerability explanation and classify the severity as \"low\", \"medium\" or \"high\" by checking if the exploitation is occured then how is it will impact the system.\n",
    "    \n",
    "    Vulnerability explanation:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "\n",
    "    \"\"\"Audit the given vulnerability explanation to determine if the severity should be categorized as \"low\", \"medium\" or \"high\".\n",
    "    \n",
    "    Vulnerability explanation:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    explanation = examples['vuln_explanation']\n",
    "    severity = examples['severity']\n",
    "    selected_system_prompt = system_prompt\n",
    "    selected_user_prompt = prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(explanation=explanation)}]\n",
    "    \n",
    "    return { \"conversations\" : conversation, 'label': severity}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"file_name\", \"vuln_title\", \"vuln_explanation\", \"vuln_recommendation\", \"vuln_code\"])\n",
    "    datasets.append(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb17a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [01:33<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 25.27%\n",
      "Std GPU util: 3.82%\n",
      "Peak GPU mem: 3.6592 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Regex pattern for extracting labels\n",
    "pattern = r\"<\\|im_start\\|>assistant\\n<think>\\n\\n</think>\\n\\n(.*?)<\\|im_end\\|>\"\n",
    "\n",
    "y_preds = [[] for _ in range(5)]\n",
    "row_times = []  # Store total time per row (inference + voting)\n",
    "final_predictions = []\n",
    "\n",
    "# Iterate through rows with normal tqdm\n",
    "for row_idx, messages_set in enumerate(tqdm(zip(*[d['test']['conversations'] for d in datasets]), desc=\"Processing rows\", total=dataset['test'].num_rows)):\n",
    "    total_start = time.time()  # Start timer for this row\n",
    "\n",
    "    preds_for_row = []\n",
    "\n",
    "    # Inference for each dataset\n",
    "    for i, messages in enumerate(messages_set):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            enable_thinking=False,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=8,\n",
    "            use_cache=True,\n",
    "            temperature=0.1,\n",
    "            min_p=0.1\n",
    "        )\n",
    "        decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "        match = re.search(pattern, decoded_output, re.DOTALL)\n",
    "        extracted_label = match.group(1).strip() if match else \"Unknown\"\n",
    "        y_preds[i].append(extracted_label)\n",
    "        preds_for_row.append(extracted_label)\n",
    "\n",
    "    # Majority voting\n",
    "    vote_count = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "    for pred in preds_for_row:\n",
    "        if pred in vote_count:\n",
    "            vote_count[pred] += 1\n",
    "    final_prediction = max(vote_count, key=vote_count.get)\n",
    "    final_predictions.append(final_prediction)\n",
    "\n",
    "    total_end = time.time()  # Stop timer for this row\n",
    "    row_times.append(total_end - total_start)\n",
    "\n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc7e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 3.27 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/gpu_utils_ours_severity.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_ours_severity.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:02<00:00, 56.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 28.08%\n",
      "Std GPU util: 25.08%\n",
      "Peak GPU mem: 0.9111 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./severity_codebert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Running on: {device}\")\n",
    "data_files = {'test': 'severity_data_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "severity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "datasets = datasets.map(lambda example: {'labels': severity_mapping[example['severity']]})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "\n",
    "thread.start()\n",
    "total_start_time = time.time()\n",
    "label_map = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "\n",
    "        vuln_exp = example['vuln_explanation']  # Adjust column name if needed\n",
    "        inputs = tokenizer(vuln_exp, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        predictions.append({\n",
    "            \"vuln_explanation\": vuln_exp,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "\n",
    "        row_end = time.time()\n",
    "\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579c60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 0.543 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_codebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_codebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d06fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:02<00:00, 59.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 28.67%\n",
      "Std GPU util: 25.03%\n",
      "Peak GPU mem: 0.9111 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"severity_graphcodebert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Running on: {device}\")\n",
    "data_files = {'test': 'severity_data_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "severity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "datasets = datasets.map(lambda example: {'labels': severity_mapping[example['severity']]})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "\n",
    "thread.start()\n",
    "total_start_time = time.time()\n",
    "label_map = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "\n",
    "        vuln_exp = example['vuln_explanation']  # Adjust column name if needed\n",
    "        inputs = tokenizer(vuln_exp, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        predictions.append({\n",
    "            \"vuln_explanation\": vuln_exp,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "\n",
    "        row_end = time.time()\n",
    "\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b6c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 0.543 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91cb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_graphcodebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_graphcodebert.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:02<00:00, 55.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 25.00%\n",
      "Std GPU util: 24.08%\n",
      "Peak GPU mem: 0.9150 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./severity_unixcoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Running on: {device}\")\n",
    "data_files = {'test': 'severity_data_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "severity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "datasets = datasets.map(lambda example: {'labels': severity_mapping[example['severity']]})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "\n",
    "thread.start()\n",
    "total_start_time = time.time()\n",
    "label_map = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "\n",
    "        vuln_exp = example['vuln_explanation']  # Adjust column name if needed\n",
    "        inputs = tokenizer(vuln_exp, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        predictions.append({\n",
    "            \"vuln_explanation\": vuln_exp,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "\n",
    "        row_end = time.time()\n",
    "\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3493f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 0.547 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_unixcoder.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_unixcoder.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/143 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:06<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 36.26%\n",
      "Std GPU util: 19.75%\n",
      "Peak GPU mem: 1.5479 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time, threading, subprocess, re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"./severity_codet5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Running on: {device}\")\n",
    "data_files = {'test': 'severity_data_test.csv'}\n",
    "datasets = load_dataset('csv', data_files=data_files)\n",
    "severity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "datasets = datasets.map(lambda example: {'labels': severity_mapping[example['severity']]})\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"nvidia-smi\",\n",
    "                \"--query-gpu=utilization.gpu,memory.used\",\n",
    "                \"--format=csv,noheader,nounits\"\n",
    "            ])\n",
    "            u, m = map(int, out.decode().strip().split(','))\n",
    "            gpu_utils.append(u)\n",
    "            gpu_mem.append(m / 1024)  # MB -> GB\n",
    "        except Exception as e:\n",
    "            print(f\"nvidia-smi polling error: {e}\")\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "\n",
    "thread.start()\n",
    "total_start_time = time.time()\n",
    "label_map = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "predictions = []\n",
    "row_times = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(datasets['test'], desc=\"Running inference\"):\n",
    "        row_start = time.time()  # Start timer for this row\n",
    "\n",
    "        vuln_exp = example['vuln_explanation']  # Adjust column name if needed\n",
    "        inputs = tokenizer(vuln_exp, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        predictions.append({\n",
    "            \"vuln_explanation\": vuln_exp,\n",
    "            \"true_label\": label_map[example['labels']],\n",
    "            \"predicted_label\": label_map[pred]\n",
    "        })\n",
    "\n",
    "        row_end = time.time()\n",
    "\n",
    "        row_times.append(row_end - row_start)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "running = False\n",
    "\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56376875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 1.18 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a011638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/times_sev_codet5.pkl', 'wb') as file:\n",
    "    pickle.dump(row_times, file)\n",
    "    \n",
    "with open('_resource_data/gpu_utils_sev_codet5.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_sev_codet5.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f5f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_gpu_util</th>\n",
       "      <th>std_gpu_util</th>\n",
       "      <th>max_gpu_mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ours_severity</td>\n",
       "      <td>25.27</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.6592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sev_codebert</td>\n",
       "      <td>28.08</td>\n",
       "      <td>25.08</td>\n",
       "      <td>0.9111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sev_codet5</td>\n",
       "      <td>36.26</td>\n",
       "      <td>19.75</td>\n",
       "      <td>1.5479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sev_ftcodellama13b</td>\n",
       "      <td>70.44</td>\n",
       "      <td>25.86</td>\n",
       "      <td>11.3604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sev_graphcodebert</td>\n",
       "      <td>28.67</td>\n",
       "      <td>25.03</td>\n",
       "      <td>0.9111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sev_unixcoder</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.08</td>\n",
       "      <td>0.9150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sev_zs_codellama13b</td>\n",
       "      <td>77.57</td>\n",
       "      <td>23.04</td>\n",
       "      <td>10.2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sev_zs_codellama34b</td>\n",
       "      <td>90.22</td>\n",
       "      <td>17.20</td>\n",
       "      <td>19.5791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  avg_gpu_util  std_gpu_util  max_gpu_mem\n",
       "0        ours_severity         25.27          3.82       3.6592\n",
       "1         sev_codebert         28.08         25.08       0.9111\n",
       "2           sev_codet5         36.26         19.75       1.5479\n",
       "3   sev_ftcodellama13b         70.44         25.86      11.3604\n",
       "4    sev_graphcodebert         28.67         25.03       0.9111\n",
       "5        sev_unixcoder         25.00         24.08       0.9150\n",
       "6  sev_zs_codellama13b         77.57         23.04      10.2900\n",
       "7  sev_zs_codellama34b         90.22         17.20      19.5791"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = [\n",
    "    \"sev_zs_codellama34b\",\n",
    "    \"sev_zs_codellama13b\",\n",
    "    \"sev_ftcodellama13b\",\n",
    "    \"ours_severity\",\n",
    "    \"sev_codebert\",\n",
    "    \"sev_graphcodebert\",\n",
    "    \"sev_unixcoder\",\n",
    "    \"sev_codet5\"\n",
    "]\n",
    "\n",
    "data_dir = \"_resource_data\"\n",
    "summary_data = {\n",
    "    \"model\": [],\n",
    "    \"avg_gpu_util\": [],\n",
    "    \"std_gpu_util\": [],\n",
    "    \"max_gpu_mem\": []\n",
    "}\n",
    "\n",
    "for model in models:    \n",
    "    with open(f\"{data_dir}/gpu_utils_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_utils = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{data_dir}/gpu_mem_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_mem = pickle.load(f)\n",
    "    \n",
    "    avg_gpu_util = np.mean(gpu_utils)\n",
    "    std_gpu_util = np.std(gpu_utils, ddof=1)\n",
    "    max_gpu_mem = np.max(gpu_mem)\n",
    "    \n",
    "    summary_data[\"model\"].append(model)\n",
    "    summary_data[\"avg_gpu_util\"].append(float(f\"{avg_gpu_util:.2f}\"))\n",
    "    summary_data[\"std_gpu_util\"].append(float(f\"{std_gpu_util:.2f}\"))\n",
    "    summary_data[\"max_gpu_mem\"].append(float(f\"{max_gpu_mem:.4f}\"))\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.sort_values(by=\"model\", inplace=True)\n",
    "df_summary.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3bbd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 06:34:24 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 06:34:24 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 06:34:26,089\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [03:24<00:00, 29.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-34b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d563aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 662 examples [00:00, 8316.22 examples/s]\n",
      "Generating val split: 142 examples [00:00, 14993.23 examples/s]\n",
      "Generating test split: 143 examples [00:00, 14962.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 5467.34 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4900.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4847.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6338.34 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 6096.18 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 6033.03 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 5000.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4900.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world.\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable. Review the code and analyze its security flaws. Just give short explanation why these function is vulnerable.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['vuln_code']\n",
    "    severity = examples['severity']\n",
    "    desc = examples['vuln_explanation']\n",
    "    assistant_prompt = desc\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code)},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_prompt}]\n",
    "    return { \"conversations\" : conversation, }\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "dataset = dataset.remove_columns([\"vuln_title\", \"vuln_explanation\", \"severity\", \"vuln_recommendation\"])\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "test_dataset = test_dataset.map(lambda row: {'answer':row['conversations'][-1]})\n",
    "test_dataset = test_dataset.map(lambda row: {'conversations':row['conversations'][:-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f449d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [56:00<00:00, 23.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 83.69%\n",
      "Std GPU util: 6.04%\n",
      "Peak GPU mem: 19.8115 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=2048, use_cache=True,\n",
    "                             temperature=0.1, min_p=0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "\n",
    "    y_pred.append(decoded_output)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84df52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 19.42 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79486e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/times_exp_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(row_times, file)\n",
    "    \n",
    "with open('_resource_data/gpu_utils_exp_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_exp_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0586b682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 07:39:15 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 07:39:15 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 07:39:17,174\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:17<00:00, 25.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-13b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 5292.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4903.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4953.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6797.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 5915.74 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 6005.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 5205.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4997.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world.\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable. Review the code and analyze its security flaws. Just give short explanation why these function is vulnerable.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['vuln_code']\n",
    "    severity = examples['severity']\n",
    "    desc = examples['vuln_explanation']\n",
    "    assistant_prompt = desc\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code)},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_prompt}]\n",
    "    return { \"conversations\" : conversation, }\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "dataset = dataset.remove_columns([\"vuln_title\", \"vuln_explanation\", \"severity\", \"vuln_recommendation\"])\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "test_dataset = test_dataset.map(lambda row: {'answer':row['conversations'][-1]})\n",
    "test_dataset = test_dataset.map(lambda row: {'conversations':row['conversations'][:-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3add45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [58:10<00:00, 24.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 49.77%\n",
      "Std GPU util: 5.97%\n",
      "Peak GPU mem: 12.3545 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=2048, use_cache=True,\n",
    "                             temperature=0.1, min_p=0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "\n",
    "    y_pred.append(decoded_output)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b6c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.332 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/times_exp_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(row_times, file)\n",
    "    \n",
    "with open('_resource_data/gpu_utils_exp_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_exp_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8fa561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-16 08:49:53 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-16 08:49:53 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 08:49:55,936\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:26<00:00,  8.75s/it]\n",
      "Unsloth 2025.5.3 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama13b_vuln_exp\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239297ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world.\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable. Review the code and analyze its security flaws.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability Explanation: \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    assistant_prompt = desc\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code)},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_prompt}]\n",
    "    return { \"conversations\" : conversation, }\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "dataset = dataset.remove_columns([\"vuln_title\", \"vuln_explanation\", \"severity\", \"vuln_recommendation\"])\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "test_dataset = test_dataset.map(lambda row: {'answer':row['conversations'][-1]})\n",
    "test_dataset = test_dataset.map(lambda row: {'conversations':row['conversations'][:-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6da815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [3:33:33<00:00, 89.60s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 39.49%\n",
      "Std GPU util: 3.91%\n",
      "Peak GPU mem: 12.7861 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations']):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=2048, use_cache=True,\n",
    "                             temperature=0.1, min_p=0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "\n",
    "    y_pred.append(decoded_output)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b420257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.395 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab116d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('_resource_data/times_exp_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(row_times, file)\n",
    "    \n",
    "with open('_resource_data/gpu_utils_exp_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_exp_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e150f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-17 03:37:47 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-17 03:37:47 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 03:37:49,191\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.3 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (2): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (3): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (5): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (7-32): 26 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (33): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (34): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (35): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 32768 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ours_explanator\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6eeb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the best solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant solidity security researcher in the world\"\"\",\n",
    "    \"\"\"You are the best AI solidity smart contract security auditor in the world\"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable. Review the code and analyze its security flaws.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"The following Solidity function contains security vulnerabilities. Examine the function and explain the specific weaknesses that make it insecure.\n",
    "    \n",
    "    Function for review:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"The provided Solidity function has security vulnerabilities. Identify and explain the security issues present in the code.\n",
    "    \n",
    "    Code for analysis:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"The Solidity function below is vulnerable. Describe the security flaws and their potential risks.\n",
    "    \n",
    "    Function under review:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"Analyze the following Solidity function. It has security vulnerabilities that need to be explained.\n",
    "    \n",
    "    Function to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(code=code)}\n",
    "    ]\n",
    "    return {\"conversations\": conversation, \"label\": desc}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"vuln_title\", \"vuln_explanation\", \"severity\", \"vuln_recommendation\"])\n",
    "    datasets.append(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, threading, subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll_gpu():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().strip().split(','))\n",
    "        gpu_utils.append(u)\n",
    "        gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "monitor_thread = threading.Thread(target=poll_gpu, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "template = \"\"\"We have several vulnerability analysis for the smart contract security audit.\n",
    "Your task is to produce a vulnerability explanation that selects and integrates only the most accurate, logical, reasonable, coherent and credible vulnerabilities from the existing analyses.\n",
    "    \n",
    "Make sure to make only one vulnerability explanation in your final answer.\n",
    "Also make sure your final answer only contain the vulnerability explanation, without any additional commentary.\n",
    "\n",
    "###Smart Contract Under Review  \n",
    "```solidity\n",
    "{code}\n",
    "````\n",
    "\n",
    "###Analysis 1\n",
    "{output_0}\n",
    "\n",
    "###Analysis 2\n",
    "{output_1}\n",
    "\n",
    "###Analysis 3\n",
    "{output_2}\n",
    "\n",
    "###Analysis 4\n",
    "{output_3}\n",
    "\n",
    "###Analysis 5:\n",
    "{output_4}\n",
    "\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    "    min_p=0,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=8192,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "proposal_times, aggregation_times, total_times = [], [], []\n",
    "results = []\n",
    "\n",
    "# num_rows = len(datasets[0]['test'])\n",
    "num_rows = 50\n",
    "\n",
    "for row_idx in tqdm(range(num_rows), desc=\"End-to-End Pipeline\"):\n",
    "    t_total_start = time.time()\n",
    "\n",
    "    proposal_outputs = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        example = dataset['test'][row_idx]\n",
    "        proposal_input = tokenizer.apply_chat_template(\n",
    "            example['conversations'],\n",
    "            tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        proposal_output = model.generate(\n",
    "            input_ids=proposal_input,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        decoded_output = tokenizer.decode(\n",
    "            proposal_output[0], skip_special_tokens=True\n",
    "        )\n",
    "        proposal_outputs.append(decoded_output)\n",
    "\n",
    "    dt_prop = time.time() - t0\n",
    "    proposal_times.append(dt_prop)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t1 = time.time()\n",
    "    aggregation_prompt = template.format(\n",
    "        code=example[\"vuln_code\"],\n",
    "        output_0=proposal_outputs[0],\n",
    "        output_1=proposal_outputs[1],\n",
    "        output_2=proposal_outputs[2],\n",
    "        output_3=proposal_outputs[3],\n",
    "        output_4=proposal_outputs[4],\n",
    "    )\n",
    "    aggregation_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in smart-contract security audits.\"},\n",
    "        {\"role\": \"user\", \"content\": aggregation_prompt},\n",
    "    ]\n",
    "    aggregation_input = tokenizer.apply_chat_template(\n",
    "        aggregation_messages,\n",
    "        tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with model.disable_adapter():\n",
    "        aggregation_output = model.generate(\n",
    "            input_ids=aggregation_input,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "    aggregation_decoded = tokenizer.decode(\n",
    "        aggregation_output[0], skip_special_tokens=True\n",
    "    )\n",
    "    dt_aggr = time.time() - t1\n",
    "    aggregation_times.append(dt_aggr)\n",
    "\n",
    "    total_times.append(time.time() - t_total_start)\n",
    "    results.append({\n",
    "        \"vuln_code\": example[\"vuln_code\"], \n",
    "        \"proposal_0\": proposal_outputs[0],\n",
    "        \"proposal_1\": proposal_outputs[1],\n",
    "        \"proposal_2\": proposal_outputs[2],\n",
    "        \"proposal_3\": proposal_outputs[3],\n",
    "        \"proposal_4\": proposal_outputs[4],\n",
    "        \"aggregated\": aggregation_decoded,\n",
    "        \"time_proposal\": dt_prop,\n",
    "        \"time_aggregation\": dt_aggr,\n",
    "        \"time_total\": total_times[-1]\n",
    "    })\n",
    "\n",
    "running = False\n",
    "monitor_thread.join()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "# df.to_csv(\"_resource_data/exp_result.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Benchmark Summary ===\")\n",
    "print(f\"Total examples: {len(df)}\")\n",
    "print(\"\\nGPU Usage:\")\n",
    "print(f\"Avg GPU Utilization: {np.mean(gpu_utils):.2f}% Â±{np.std(gpu_utils, ddof=1):.2f}%\")\n",
    "print(f\"Peak VRAM Usage: {np.max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d682b6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 5.502 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_exp_ours.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_exp_ours.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[exp_zs_codellama34b] Total time for 50 rows: 1170.71s\n",
      "[exp_zs_codellama34b] Cropping GPU data to 5853 samples (~1170.60s)\n",
      "[exp_zs_codellama13b] Total time for 50 rows: 1207.19s\n",
      "[exp_zs_codellama13b] Cropping GPU data to 6035 samples (~1207.00s)\n",
      "[exp_ftcodellama13b] Total time for 50 rows: 3846.81s\n",
      "[exp_ftcodellama13b] Cropping GPU data to 19234 samples (~3846.80s)\n",
      "[exp_ours] Using full GPU monitoring data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_gpu_util</th>\n",
       "      <th>std_gpu_util</th>\n",
       "      <th>max_gpu_mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_ftcodellama13b</td>\n",
       "      <td>38.91</td>\n",
       "      <td>4.11</td>\n",
       "      <td>11.1221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_ours</td>\n",
       "      <td>36.76</td>\n",
       "      <td>11.65</td>\n",
       "      <td>9.5107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_zs_codellama13b</td>\n",
       "      <td>49.20</td>\n",
       "      <td>6.03</td>\n",
       "      <td>10.5303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_zs_codellama34b</td>\n",
       "      <td>82.90</td>\n",
       "      <td>6.65</td>\n",
       "      <td>19.5928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  avg_gpu_util  std_gpu_util  max_gpu_mem\n",
       "0   exp_ftcodellama13b         38.91          4.11      11.1221\n",
       "1             exp_ours         36.76         11.65       9.5107\n",
       "2  exp_zs_codellama13b         49.20          6.03      10.5303\n",
       "3  exp_zs_codellama34b         82.90          6.65      19.5928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = [\n",
    "    \"exp_zs_codellama34b\",\n",
    "    \"exp_zs_codellama13b\",\n",
    "    \"exp_ftcodellama13b\",\n",
    "    \"exp_ours\",\n",
    "]\n",
    "\n",
    "data_dir = \"_resource_data\"\n",
    "summary_data = {\n",
    "    \"model\": [],\n",
    "    \"avg_gpu_util\": [],\n",
    "    \"std_gpu_util\": [],\n",
    "    \"max_gpu_mem\": []\n",
    "}\n",
    "\n",
    "sampling_interval = 0.2 \n",
    "\n",
    "for model in models:\n",
    "    with open(f\"{data_dir}/gpu_utils_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_utils = pickle.load(f)\n",
    "    with open(f\"{data_dir}/gpu_mem_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_mem = pickle.load(f)\n",
    "    \n",
    "    if model != \"exp_ours\":\n",
    "        with open(f\"{data_dir}/times_{model}.pkl\", \"rb\") as f:\n",
    "            row_times = pickle.load(f)\n",
    "\n",
    "        total_time = sum(row_times[:50])\n",
    "        \n",
    "        num_samples = int(total_time / sampling_interval)\n",
    "\n",
    "        gpu_utils = gpu_utils[:num_samples]\n",
    "        gpu_mem = gpu_mem[:num_samples]\n",
    "\n",
    "        row_times = row_times[:50]\n",
    "\n",
    "    else:\n",
    "        print(f\"[{model}] Using full GPU monitoring data\")\n",
    "\n",
    "    avg_gpu_util = np.mean(gpu_utils)\n",
    "    std_gpu_util = np.std(gpu_utils, ddof=1)\n",
    "    max_gpu_mem = np.max(gpu_mem)\n",
    "    \n",
    "    summary_data[\"model\"].append(model)\n",
    "    summary_data[\"avg_gpu_util\"].append(float(f\"{avg_gpu_util:.2f}\"))\n",
    "    summary_data[\"std_gpu_util\"].append(float(f\"{std_gpu_util:.2f}\"))\n",
    "    summary_data[\"max_gpu_mem\"].append(float(f\"{max_gpu_mem:.4f}\"))\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.sort_values(by=\"model\", inplace=True)\n",
    "df_summary.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3497eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-17 09:29:21 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-17 09:29:21 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 09:29:23,101\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [03:20<00:00, 28.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-34b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-34b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 5114.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4548.44 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4660.56 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 7799.41 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 6437.78 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 6523.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable and there was an explanation about the vulnerability. Give your patch recommendation based on that vulnerability. Just straight to the recommendation without any additional information.\n",
    "    \n",
    "    This is the vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "This is the vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    recom = examples['vuln_recommendation']\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code, explanation=desc)}]\n",
    "    return { \"conversations\" : conversation, \"label\": recom}\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c40adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [31:37<00:00, 37.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 88.55%\n",
      "Std GPU util: 7.01%\n",
      "Peak GPU mem: 19.7354 GB\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations'][:50]):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=2048, use_cache=True,\n",
    "                             temperature=0.1, min_p=0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "\n",
    "    y_pred.append(decoded_output)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f37816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 19.344 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_rec_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_rec_zs_codellama34b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c461e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-17 10:14:28 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-17 10:14:28 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 10:14:30,517\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:16<00:00, 25.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codellama/CodeLlama-13b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d93476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 7217.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 5764.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 6156.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable and there was an explanation about the vulnerability. Give your patch recommendation based on that vulnerability. Just straight to the recommendation without any additional information.\n",
    "    \n",
    "    This is the vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "This is the vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    recom = examples['vuln_recommendation']\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code, explanation=desc)}]\n",
    "    return { \"conversations\" : conversation, \"label\": recom}\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8d98a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [23:50<00:00, 28.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 49.27%\n",
      "Std GPU util: 6.66%\n",
      "Peak GPU mem: 12.4092 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations'][:50]):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=2048, use_cache=True,\n",
    "                             temperature=0.1, min_p=0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "\n",
    "    y_pred.append(decoded_output)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecafe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.332 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_rec_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_rec_zs_codellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bf4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-17 10:55:47 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-17 10:55:47 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 10:55:49,796\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:26<00:00,  8.69s/it]\n",
      "Unsloth 2025.5.3 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 16384 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama13b_recommendation\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51726e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 5464.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 4690.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4742.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 6752.14 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 6048.51 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 6040.56 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4929.81 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 4785.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable and there was an explanation about the vulnerability. Give your patch recommendation based on that vulnerability. Just straight to the recommendation without any additional information.\n",
    "    \n",
    "    This is the vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "This is the vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    recom = examples['vuln_recommendation']\n",
    "    selected_system_prompt = random.choice(SYSTEM_PROMPT)\n",
    "    selected_user_prompt = random.choice(PROMPTS) \n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code, explanation=desc)}]\n",
    "    return { \"conversations\" : conversation, \"label\": recom}\n",
    "\n",
    "dataset = dataset.map(make_conversational)\n",
    "\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f941ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [1:02:36<00:00, 75.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg GPU util: 39.75%\n",
      "Std GPU util: 4.06%\n",
      "Peak GPU mem: 11.1221 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().split(','))\n",
    "        gpu_utils.append(u); gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "thread = threading.Thread(target=poll, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Define regex pattern to extract the assistant's response\n",
    "pattern = r\"\\[/INST](.*?)</s>\"\n",
    "\n",
    "y_pred = []\n",
    "row_times = []\n",
    "\n",
    "for messages in tqdm(test_dataset['conversations'][:50]):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=2048, use_cache=True,\n",
    "                             temperature=0.1, min_p=0.1)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "    \n",
    "    # Stop timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    row_times.append(elapsed_time)\n",
    "\n",
    "    y_pred.append(decoded_output)\n",
    "    \n",
    "running = False\n",
    "thread.join()\n",
    "\n",
    "import numpy as np\n",
    "std_util = np.std(gpu_utils, ddof=1) \n",
    "\n",
    "print(f\"Avg GPU util: {sum(gpu_utils)/len(gpu_utils):.2f}%\")\n",
    "print(f\"Std GPU util: {std_util:.2f}%\")\n",
    "print(f\"Peak GPU mem: {max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cedc106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 12.332 GB.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a36873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_rec_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_rec_ftcodellama13b.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-17 15:22:25 [importing.py:53] Triton module has been replaced with a placeholder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-17 15:22:25 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 15:22:27,471\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.3 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (2): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (3): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (5): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (7-32): 26 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (33): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (34): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (35): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 32768 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ours_recom\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train.csv',\n",
    "    'val': 'vuln_data_val.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the best solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant solidity security researcher in the world\"\"\",\n",
    "    \"\"\"You are the best AI solidity smart contract security auditor in the world\"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable and there was an explanation about the vulnerability. Give your patch recommendation based on that vulnerability.\n",
    "    \n",
    "    This is the vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "This is the vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"The following Solidity function contains security vulnerabilities. Based on vulnerability explanation give the recommendation to patch the security issues.\n",
    "    \n",
    "    Functions that has vulnerability:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability explanation of the function: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"The provided Solidity function has security vulnerabilities. Your task is to give recommendation to close the security issue.\n",
    "    \n",
    "    Vulnerable code:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"The Solidity function below is vulnerable. Give your best recommendation to close that security issue based on vulnerability explanation.\n",
    "    \n",
    "    Functions that vulnerable:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"Analyze the following Solidity function. It has security vulnerabilities that need to be patched. Give your best recommendation.\n",
    "    \n",
    "    Vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "The vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational_vote(examples, system_prompt, prompt):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    recom = examples['vuln_recommendation']\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": prompt.format(code=code, explanation=desc)}]\n",
    "    return {\"conversations\": conversation, \"label\": recom}\n",
    "\n",
    "# Generate 5 datasets using different prompts\n",
    "datasets = []\n",
    "for i in range(len(PROMPTS)):\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational_vote(ex, SYSTEM_PROMPT[i], PROMPTS[i]))\n",
    "    new_dataset = new_dataset.remove_columns([\"vuln_title\", \"severity\"])\n",
    "    datasets.append(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f8f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:   0%|                                                                             | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:   2%|â–ˆâ–Ž                                                                | 1/50 [04:41<3:49:39, 281.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:   4%|â–ˆâ–ˆâ–‹                                                               | 2/50 [09:04<3:36:20, 270.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:   6%|â–ˆâ–ˆâ–ˆâ–‰                                                              | 3/50 [12:25<3:07:14, 239.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                            | 4/50 [17:07<3:16:13, 255.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 5/50 [21:25<3:12:23, 256.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                          | 6/50 [24:51<2:55:30, 239.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                        | 7/50 [29:12<2:56:47, 246.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                       | 8/50 [34:16<3:05:16, 264.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                      | 9/50 [38:42<3:01:08, 265.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 10/50 [43:11<2:57:34, 266.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                  | 11/50 [47:41<2:53:54, 267.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 12/50 [51:43<2:44:32, 259.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                | 13/50 [55:17<2:31:36, 245.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 14/50 [58:57<2:22:52, 238.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                            | 15/50 [1:02:33<2:14:57, 231.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 16/50 [1:06:06<2:07:58, 225.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 17/50 [1:08:31<1:50:46, 201.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                        | 18/50 [1:12:25<1:52:39, 211.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 19/50 [1:15:55<1:49:04, 211.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 20/50 [1:19:49<1:48:52, 217.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 21/50 [1:24:04<1:50:38, 228.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 22/50 [1:26:31<1:35:27, 204.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 23/50 [1:29:27<1:28:07, 195.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 24/50 [1:32:17<1:21:27, 187.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 25/50 [1:36:06<1:23:28, 200.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 26/50 [1:39:17<1:19:05, 197.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 27/50 [1:43:24<1:21:25, 212.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           | 28/50 [1:47:43<1:23:01, 226.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 29/50 [1:51:23<1:18:35, 224.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 30/50 [1:55:18<1:15:49, 227.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 31/50 [1:59:50<1:16:15, 240.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 32/50 [2:05:52<1:23:11, 277.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 33/50 [2:11:49<1:25:21, 301.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 34/50 [2:16:01<1:16:23, 286.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 35/50 [2:20:51<1:11:50, 287.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 36/50 [2:25:34<1:06:45, 286.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 37/50 [2:29:44<59:38, 275.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 38/50 [2:33:24<51:45, 258.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 39/50 [2:37:56<48:11, 262.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 40/50 [2:41:00<39:51, 239.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 41/50 [2:44:57<35:44, 238.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 42/50 [2:49:42<33:39, 252.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 43/50 [2:54:46<31:15, 267.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 44/50 [2:58:50<26:04, 260.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 45/50 [3:03:38<22:25, 269.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/50 [3:06:35<16:04, 241.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 47/50 [3:10:14<11:43, 234.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/50 [3:13:02<07:09, 214.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 49/50 [3:18:30<04:08, 248.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [3:22:17<00:00, 242.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "End-to-End Pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [3:22:17<00:00, 242.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmark Summary ===\n",
      "\n",
      "GPU Usage:\n",
      "Avg GPU Utilization: 36.27% Â±12.16%\n",
      "Peak VRAM Usage: 9.2979 GB\n"
     ]
    }
   ],
   "source": [
    "import time, threading, subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "gpu_utils, gpu_mem = [], []\n",
    "running = True\n",
    "\n",
    "def poll_gpu():\n",
    "    while running:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=utilization.gpu,memory.used\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ])\n",
    "        u, m = map(int, out.decode().strip().split(','))\n",
    "        gpu_utils.append(u)\n",
    "        gpu_mem.append(m / 1024)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "monitor_thread = threading.Thread(target=poll_gpu, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "template = \"\"\"We have several vulnerability mitigation/recommendation for the smart contract security audit.\n",
    "Your task is to produce a recommendation/mitigation steps that selects and integrates only the most accurate, logical, reasonable, coherent and credible recommendation from the existing analyses.\n",
    "Make sure your final answer only contain the final recommendation/mitigation answer, without any additional commentary.\n",
    "\n",
    "###Vulnerability Explanation\n",
    "{explanation}\n",
    "\n",
    "###Smart Contract Code\n",
    "{code}\n",
    "\n",
    "###Analysis 1\n",
    "{output_0}\n",
    "\n",
    "###Analysis 2\n",
    "{output_1}\n",
    "\n",
    "###Analysis 3\n",
    "{output_2}\n",
    "\n",
    "###Analysis 4\n",
    "{output_3}\n",
    "\n",
    "###Analysis 5:\n",
    "{output_4}\n",
    "\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_k=20,\n",
    "    top_p=0.95,\n",
    "    min_p=0,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=8192,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "proposal_times, aggregation_times, total_times = [], [], []\n",
    "results = []\n",
    "\n",
    "# num_rows = len(datasets[0]['test'])\n",
    "num_rows = 50\n",
    "\n",
    "for row_idx in tqdm(range(num_rows), desc=\"End-to-End Pipeline\"):\n",
    "    t_total_start = time.time()\n",
    "\n",
    "    proposal_outputs = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        example = dataset['test'][row_idx]\n",
    "        proposal_input = tokenizer.apply_chat_template(\n",
    "            example['conversations'],\n",
    "            tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        proposal_output = model.generate(\n",
    "            input_ids=proposal_input,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        decoded_output = tokenizer.decode(\n",
    "            proposal_output[0], skip_special_tokens=True\n",
    "        )\n",
    "        proposal_outputs.append(decoded_output)\n",
    "\n",
    "    dt_prop = time.time() - t0\n",
    "    proposal_times.append(dt_prop)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    t1 = time.time()\n",
    "    aggregation_prompt = template.format(\n",
    "        code=example[\"vuln_code\"],\n",
    "        explanation=example[\"vuln_explanation\"],\n",
    "        output_0=proposal_outputs[0],\n",
    "        output_1=proposal_outputs[1],\n",
    "        output_2=proposal_outputs[2],\n",
    "        output_3=proposal_outputs[3],\n",
    "        output_4=proposal_outputs[4],\n",
    "    )\n",
    "    aggregation_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in smart-contract security audits.\"},\n",
    "        {\"role\": \"user\", \"content\": aggregation_prompt},\n",
    "    ]\n",
    "    aggregation_input = tokenizer.apply_chat_template(\n",
    "        aggregation_messages,\n",
    "        tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with model.disable_adapter():\n",
    "        aggregation_output = model.generate(\n",
    "            input_ids=aggregation_input,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "    aggregation_decoded = tokenizer.decode(\n",
    "        aggregation_output[0], skip_special_tokens=True\n",
    "    )\n",
    "    dt_aggr = time.time() - t1\n",
    "    aggregation_times.append(dt_aggr)\n",
    "\n",
    "    total_times.append(time.time() - t_total_start)\n",
    "    results.append({\n",
    "        \"vuln_code\": example[\"vuln_code\"], \n",
    "        \"vuln_explanation\": example[\"vuln_explanation\"], \n",
    "        \"proposal_0\": proposal_outputs[0],\n",
    "        \"proposal_1\": proposal_outputs[1],\n",
    "        \"proposal_2\": proposal_outputs[2],\n",
    "        \"proposal_3\": proposal_outputs[3],\n",
    "        \"proposal_4\": proposal_outputs[4],\n",
    "        \"aggregated\": aggregation_decoded,\n",
    "        \"time_proposal\": dt_prop,\n",
    "        \"time_aggregation\": dt_aggr,\n",
    "        \"time_total\": total_times[-1]\n",
    "    })\n",
    "\n",
    "running = False\n",
    "monitor_thread.join()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "# df.to_csv(\"_resource_data/recom_result.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Benchmark Summary ===\")\n",
    "print(\"\\nGPU Usage:\")\n",
    "print(f\"Avg GPU Utilization: {np.mean(gpu_utils):.2f}% Â±{np.std(gpu_utils, ddof=1):.2f}%\")\n",
    "print(f\"Peak VRAM Usage: {np.max(gpu_mem):.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 5.736 GB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open('_resource_data/gpu_utils_rec_ours.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_utils, file)\n",
    "    \n",
    "with open('_resource_data/gpu_mem_rec_ours.pkl', 'wb') as file:\n",
    "    pickle.dump(gpu_mem, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67e7bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_gpu_util</th>\n",
       "      <th>std_gpu_util</th>\n",
       "      <th>max_gpu_mem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec_ftcodellama13b</td>\n",
       "      <td>39.75</td>\n",
       "      <td>4.06</td>\n",
       "      <td>11.1221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec_ours</td>\n",
       "      <td>36.27</td>\n",
       "      <td>12.16</td>\n",
       "      <td>9.2979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec_zs_codellama13b</td>\n",
       "      <td>49.27</td>\n",
       "      <td>6.66</td>\n",
       "      <td>12.4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rec_zs_codellama34b</td>\n",
       "      <td>88.55</td>\n",
       "      <td>7.01</td>\n",
       "      <td>19.7354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  avg_gpu_util  std_gpu_util  max_gpu_mem\n",
       "0   rec_ftcodellama13b         39.75          4.06      11.1221\n",
       "1             rec_ours         36.27         12.16       9.2979\n",
       "2  rec_zs_codellama13b         49.27          6.66      12.4092\n",
       "3  rec_zs_codellama34b         88.55          7.01      19.7354"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = [\n",
    "    \"rec_zs_codellama34b\",\n",
    "    \"rec_zs_codellama13b\",\n",
    "    \"rec_ftcodellama13b\",\n",
    "    \"rec_ours\",\n",
    "]\n",
    "\n",
    "data_dir = \"_resource_data\"\n",
    "summary_data = {\n",
    "    \"model\": [],\n",
    "    \"avg_gpu_util\": [],\n",
    "    \"std_gpu_util\": [],\n",
    "    \"max_gpu_mem\": []\n",
    "}\n",
    "\n",
    "sampling_interval = 0.2  # seconds between GPU samples\n",
    "\n",
    "for model in models:\n",
    "    with open(f\"{data_dir}/gpu_utils_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_utils = pickle.load(f)\n",
    "    with open(f\"{data_dir}/gpu_mem_{model}.pkl\", \"rb\") as f:\n",
    "        gpu_mem = pickle.load(f)\n",
    "    \n",
    "    avg_gpu_util = np.mean(gpu_utils)\n",
    "    std_gpu_util = np.std(gpu_utils, ddof=1)\n",
    "    max_gpu_mem = np.max(gpu_mem)\n",
    "    \n",
    "    summary_data[\"model\"].append(model)\n",
    "    summary_data[\"avg_gpu_util\"].append(float(f\"{avg_gpu_util:.2f}\"))\n",
    "    summary_data[\"std_gpu_util\"].append(float(f\"{std_gpu_util:.2f}\"))\n",
    "    summary_data[\"max_gpu_mem\"].append(float(f\"{max_gpu_mem:.4f}\"))\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary.sort_values(by=\"model\", inplace=True)\n",
    "df_summary.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1600c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
