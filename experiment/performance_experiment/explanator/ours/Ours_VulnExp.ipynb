{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcae7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:28:57.975634Z",
     "iopub.status.busy": "2025-06-06T18:28:57.975336Z",
     "iopub.status.idle": "2025-06-06T18:28:57.986207Z",
     "shell.execute_reply": "2025-06-06T18:28:57.985429Z"
    }
   },
   "outputs": [],
   "source": [
    "LORA_RANK=64\n",
    "LORA_DROPOUT=0.2\n",
    "ALL_TARGET_MODULES='qkvogud'\n",
    "RSLORA=True\n",
    "MODEL_SEED=42\n",
    "EVAL_STEPS=200\n",
    "BATCH_SIZE=8\n",
    "GRADIENT_ACCUMULATION=1\n",
    "EPOCH=5\n",
    "TRAINING_SEED=3407\n",
    "WEIGHT_DECAY=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7da50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:28:57.989665Z",
     "iopub.status.busy": "2025-06-06T18:28:57.989381Z",
     "iopub.status.idle": "2025-06-06T18:29:25.214638Z",
     "shell.execute_reply": "2025-06-06T18:29:25.213741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-06 18:29:06 [importing.py:53] Triton module has been replaced with a placeholder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-06 18:29:06 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 18:29:08,282\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1/2 [00:04<00:04,  4.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen3-4B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104f7e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:25.218811Z",
     "iopub.status.busy": "2025-06-06T18:29:25.218250Z",
     "iopub.status.idle": "2025-06-06T18:29:25.223460Z",
     "shell.execute_reply": "2025-06-06T18:29:25.222734Z"
    }
   },
   "outputs": [],
   "source": [
    "FONT_TO_MODULE = {\n",
    "    \"q\": \"q_proj\",\n",
    "    \"k\": \"k_proj\",\n",
    "    \"v\": \"v_proj\",\n",
    "    \"o\": \"o_proj\",\n",
    "    \"g\": \"gate_proj\",\n",
    "    \"u\": \"up_proj\",\n",
    "    \"d\": \"down_proj\"\n",
    "}\n",
    "\n",
    "# Example: ALL_TARGET_MODULES = \"gqk\"\n",
    "target_modules = [FONT_TO_MODULE[char] for char in ALL_TARGET_MODULES if char in FONT_TO_MODULE]\n",
    "\n",
    "# If you want default behavior (all modules) when empty\n",
    "if not target_modules:\n",
    "    target_modules = list(FONT_TO_MODULE.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a1e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:25.226514Z",
     "iopub.status.busy": "2025-06-06T18:29:25.226248Z",
     "iopub.status.idle": "2025-06-06T18:29:30.827294Z",
     "shell.execute_reply": "2025-06-06T18:29:30.826219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.3 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "lora_rank = LORA_RANK\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = target_modules,\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = LORA_DROPOUT, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = MODEL_SEED,\n",
    "    use_rslora = RSLORA,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "model_setup = f\"\"\"lora_rank = {LORA_RANK}\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = {target_modules},\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = {LORA_DROPOUT}, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = {MODEL_SEED},\n",
    "    use_rslora = {RSLORA},  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8032f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:30.831301Z",
     "iopub.status.busy": "2025-06-06T18:29:30.831020Z",
     "iopub.status.idle": "2025-06-06T18:29:31.877611Z",
     "shell.execute_reply": "2025-06-06T18:29:31.876598Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train_with_CoT.csv',\n",
    "    'val': 'vuln_data_val_with_CoT.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cdeb94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:31.881351Z",
     "iopub.status.busy": "2025-06-06T18:29:31.881031Z",
     "iopub.status.idle": "2025-06-06T18:29:32.026294Z",
     "shell.execute_reply": "2025-06-06T18:29:32.025594Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the best solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant solidity security researcher in the world\"\"\",\n",
    "    \"\"\"You are the best AI solidity smart contract security auditor in the world\"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable. Review the code and analyze its security flaws.\n",
    "    \n",
    "    This is the function we need to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"The following Solidity function contains security vulnerabilities. Examine the function and explain the specific weaknesses that make it insecure.\n",
    "    \n",
    "    Function for review:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"The provided Solidity function has security vulnerabilities. Identify and explain the security issues present in the code.\n",
    "    \n",
    "    Code for analysis:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"The Solidity function below is vulnerable. Describe the security flaws and their potential risks.\n",
    "    \n",
    "    Function under review:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "\n",
    "    \"\"\"Analyze the following Solidity function. It has security vulnerabilities that need to be explained.\n",
    "    \n",
    "    Function to audit:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability : \"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples, reasoning_idx, system_prompt, prompt):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    reasoning_content = examples[f'CoT_{reasoning_idx}']\n",
    "    selected_system_prompt = system_prompt\n",
    "    selected_user_prompt = prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code)},\n",
    "                    {\"role\": \"assistant\", \"reasoning_content\":reasoning_content, \"content\": desc}]\n",
    "    \n",
    "    return { \"conversations\" : conversation}\n",
    "\n",
    "# List of splits you want to process\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "# Dictionary to store concatenated datasets for each split\n",
    "concatenated_datasets = {split: [] for split in splits}\n",
    "\n",
    "# Process each dataset\n",
    "for i in range(5):\n",
    "    system_prompt = SYSTEM_PROMPT[i]\n",
    "    prompt = PROMPTS[i]\n",
    "    \n",
    "    # Create a new dataset using map\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational(ex, i, system_prompt, prompt))\n",
    "    \n",
    "    # Append the correct split of the new dataset to the corresponding list\n",
    "    for split in splits:\n",
    "        concatenated_datasets[split].append(new_dataset[split])\n",
    "\n",
    "# Now concatenate all datasets for each split\n",
    "dataset = DatasetDict({\n",
    "    split: concatenate_datasets(concatenated_datasets[split]).shuffle(seed=42) for split in splits\n",
    "})\n",
    "\n",
    "dataset = dataset.remove_columns([\"vuln_title\", \"vuln_explanation\", \"severity\", \"vuln_recommendation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d965d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:32.029298Z",
     "iopub.status.busy": "2025-06-06T18:29:32.029018Z",
     "iopub.status.idle": "2025-06-06T18:29:32.368811Z",
     "shell.execute_reply": "2025-06-06T18:29:32.367823Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(\n",
    "        convo, \n",
    "        tokenize = False, \n",
    "        add_generation_prompt = False,\n",
    "        enable_thinking=True) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9296bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:32.372042Z",
     "iopub.status.busy": "2025-06-06T18:29:32.371776Z",
     "iopub.status.idle": "2025-06-06T18:29:32.380494Z",
     "shell.execute_reply": "2025-06-06T18:29:32.379605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are the greatest AI assistant solidity security researcher in the world',\n",
       "  'reasoning_content': None,\n",
       "  'role': 'system'},\n",
       " {'content': 'The Solidity function below is vulnerable. Describe the security flaws and their potential risks.\\n    \\n    Function under review:\\n    ```solidity\\n    function verify(\\n        address signer,\\n        MetaTransaction memory metaTx,\\n        bytes32 sigR,\\n        bytes32 sigS,\\n        uint8 sigV\\n    ) internal view returns (bool) {\\n        require(signer != address(0), \"NativeMetaTransaction: INVALID_SIGNER\");\\n        return\\n            signer ==\\n            ecrecover(\\n                toTypedMessageHash(hashMetaTransaction(metaTx)),\\n                sigV,\\n                sigR,\\n                sigS\\n            );\\n    }\\n    ```\\n    \\nVulnerability : ',\n",
       "  'reasoning_content': None,\n",
       "  'role': 'user'},\n",
       " {'content': \"The `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user's nonce is used only once, rejecting malleable signatures is considered a best practice.\",\n",
       "  'reasoning_content': \"Okay, let's take a look at the `verify` function. It's supposed to verify a signature against a meta-transaction, right? The function uses `ecrecover` to get the signer from the signature components `sigR`, `sigS`, and `sigV`. \\n\\nHmm, the `ecrecover` function is part of the EVM and is used to recover the address that signed a message. But wait, I remember that `ecrecover` can return multiple valid addresses for a single signature due to the way ECDSA signatures work. That's because of the malleability of signaturesâ€”there are multiple valid `(r, s)` pairs that can represent the same signature.\\n\\nSo, if `ecrecover` can return different addresses for the same signature, then the `verify` function might not be checking for all possible valid signatures. That could allow an attacker to forge a signature that passes the check even if it wasn't originally signed by the intended address. \\n\\nBut the function is comparing the recovered address directly to the `signer` parameter. If the signature is malleable and `ecrecover` returns a different address than the one provided, the check would fail. However, if the signature is modified in a way that still results in the same recovered address, it could be a replay of a previous signature.\\n\\nWait, the note mentions that replay attacks aren't possible here because each user's nonce is used only once. But the core issue is that `ecrecover` is malleable, and not rejecting malleable signatures is considered a best practice. So even if replay attacks aren't a concern here, the function isn't handling malleable signatures properly, which is a vulnerability.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bae3486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:32.383551Z",
     "iopub.status.busy": "2025-06-06T18:29:32.383277Z",
     "iopub.status.idle": "2025-06-06T18:29:32.388617Z",
     "shell.execute_reply": "2025-06-06T18:29:32.387664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are the greatest AI assistant solidity security researcher in the world<|im_end|>\\n<|im_start|>user\\nThe Solidity function below is vulnerable. Describe the security flaws and their potential risks.\\n    \\n    Function under review:\\n    ```solidity\\n    function verify(\\n        address signer,\\n        MetaTransaction memory metaTx,\\n        bytes32 sigR,\\n        bytes32 sigS,\\n        uint8 sigV\\n    ) internal view returns (bool) {\\n        require(signer != address(0), \"NativeMetaTransaction: INVALID_SIGNER\");\\n        return\\n            signer ==\\n            ecrecover(\\n                toTypedMessageHash(hashMetaTransaction(metaTx)),\\n                sigV,\\n                sigR,\\n                sigS\\n            );\\n    }\\n    ```\\n    \\nVulnerability : <|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let\\'s take a look at the `verify` function. It\\'s supposed to verify a signature against a meta-transaction, right? The function uses `ecrecover` to get the signer from the signature components `sigR`, `sigS`, and `sigV`. \\n\\nHmm, the `ecrecover` function is part of the EVM and is used to recover the address that signed a message. But wait, I remember that `ecrecover` can return multiple valid addresses for a single signature due to the way ECDSA signatures work. That\\'s because of the malleability of signaturesâ€”there are multiple valid `(r, s)` pairs that can represent the same signature.\\n\\nSo, if `ecrecover` can return different addresses for the same signature, then the `verify` function might not be checking for all possible valid signatures. That could allow an attacker to forge a signature that passes the check even if it wasn\\'t originally signed by the intended address. \\n\\nBut the function is comparing the recovered address directly to the `signer` parameter. If the signature is malleable and `ecrecover` returns a different address than the one provided, the check would fail. However, if the signature is modified in a way that still results in the same recovered address, it could be a replay of a previous signature.\\n\\nWait, the note mentions that replay attacks aren\\'t possible here because each user\\'s nonce is used only once. But the core issue is that `ecrecover` is malleable, and not rejecting malleable signatures is considered a best practice. So even if replay attacks aren\\'t a concern here, the function isn\\'t handling malleable signatures properly, which is a vulnerability.\\n</think>\\n\\nThe `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user\\'s nonce is used only once, rejecting malleable signatures is considered a best practice.<|im_end|>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa3f1445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:32.391870Z",
     "iopub.status.busy": "2025-06-06T18:29:32.391605Z",
     "iopub.status.idle": "2025-06-06T18:29:32.522631Z",
     "shell.execute_reply": "2025-06-06T18:29:32.521845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Max Length =  1729\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset['train'].map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"conversations\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "# print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Train Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset['train'] = dataset['train'].select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b161b795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:32.525714Z",
     "iopub.status.busy": "2025-06-06T18:29:32.525424Z",
     "iopub.status.idle": "2025-06-06T18:29:32.653599Z",
     "shell.execute_reply": "2025-06-06T18:29:32.652834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Max Length =  1806\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset['val'].map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"conversations\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Val Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset['val'] = dataset['val'].select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38290189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:32.656782Z",
     "iopub.status.busy": "2025-06-06T18:29:32.656510Z",
     "iopub.status.idle": "2025-06-06T18:29:33.095738Z",
     "shell.execute_reply": "2025-06-06T18:29:33.094689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=EVAL_STEPS,\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size = BATCH_SIZE,\n",
    "        gradient_accumulation_steps = GRADIENT_ACCUMULATION, \n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = EPOCH, \n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", \n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = TRAINING_SEED,\n",
    "        load_best_model_at_end=True,\n",
    "        output_dir = \"outputs_vuln_exp_v2\",\n",
    "        report_to = \"none\", \n",
    "    ),\n",
    ")\n",
    "\n",
    "training_setup = f\"\"\"from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps={EVAL_STEPS},\n",
    "        save_steps={EVAL_STEPS},\n",
    "#         per_device_train_batch_size = 1,\n",
    "        per_device_train_batch_size = {BATCH_SIZE},\n",
    "        per_device_eval_batch_size = {BATCH_SIZE},\n",
    "        gradient_accumulation_steps = {GRADIENT_ACCUMULATION}, # Fixed major bug in latest Unsloth\n",
    "#         warmup_steps = 5,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = {EPOCH}, # Set this for 1 full training run.\n",
    "#         max_steps = 30,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Save more memory\n",
    "        weight_decay = {WEIGHT_DECAY},\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = {TRAINING_SEED},\n",
    "        load_best_model_at_end=True,\n",
    "        output_dir = \"outputs_vuln_exp_v2\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3275a622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:33.099147Z",
     "iopub.status.busy": "2025-06-06T18:29:33.098874Z",
     "iopub.status.idle": "2025-06-06T18:29:34.631805Z",
     "shell.execute_reply": "2025-06-06T18:29:34.630713Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2775098d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:34.635341Z",
     "iopub.status.busy": "2025-06-06T18:29:34.635056Z",
     "iopub.status.idle": "2025-06-06T18:29:34.643154Z",
     "shell.execute_reply": "2025-06-06T18:29:34.642427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are the greatest AI assistant solidity security researcher in the world<|im_end|>\\n<|im_start|>user\\nThe Solidity function below is vulnerable. Describe the security flaws and their potential risks.\\n    \\n    Function under review:\\n    ```solidity\\n    function verify(\\n        address signer,\\n        MetaTransaction memory metaTx,\\n        bytes32 sigR,\\n        bytes32 sigS,\\n        uint8 sigV\\n    ) internal view returns (bool) {\\n        require(signer != address(0), \"NativeMetaTransaction: INVALID_SIGNER\");\\n        return\\n            signer ==\\n            ecrecover(\\n                toTypedMessageHash(hashMetaTransaction(metaTx)),\\n                sigV,\\n                sigR,\\n                sigS\\n            );\\n    }\\n    ```\\n    \\nVulnerability : <|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let\\'s take a look at the `verify` function. It\\'s supposed to verify a signature against a meta-transaction, right? The function uses `ecrecover` to get the signer from the signature components `sigR`, `sigS`, and `sigV`. \\n\\nHmm, the `ecrecover` function is part of the EVM and is used to recover the address that signed a message. But wait, I remember that `ecrecover` can return multiple valid addresses for a single signature due to the way ECDSA signatures work. That\\'s because of the malleability of signaturesâ€”there are multiple valid `(r, s)` pairs that can represent the same signature.\\n\\nSo, if `ecrecover` can return different addresses for the same signature, then the `verify` function might not be checking for all possible valid signatures. That could allow an attacker to forge a signature that passes the check even if it wasn\\'t originally signed by the intended address. \\n\\nBut the function is comparing the recovered address directly to the `signer` parameter. If the signature is malleable and `ecrecover` returns a different address than the one provided, the check would fail. However, if the signature is modified in a way that still results in the same recovered address, it could be a replay of a previous signature.\\n\\nWait, the note mentions that replay attacks aren\\'t possible here because each user\\'s nonce is used only once. But the core issue is that `ecrecover` is malleable, and not rejecting malleable signatures is considered a best practice. So even if replay attacks aren\\'t a concern here, the function isn\\'t handling malleable signatures properly, which is a vulnerability.\\n</think>\\n\\nThe `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user\\'s nonce is used only once, rejecting malleable signatures is considered a best practice.<|im_end|>\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856aa8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:34.646300Z",
     "iopub.status.busy": "2025-06-06T18:29:34.646035Z",
     "iopub.status.idle": "2025-06-06T18:29:34.654016Z",
     "shell.execute_reply": "2025-06-06T18:29:34.653318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                 <think>\\nOkay, let's take a look at the `verify` function. It's supposed to verify a signature against a meta-transaction, right? The function uses `ecrecover` to get the signer from the signature components `sigR`, `sigS`, and `sigV`. \\n\\nHmm, the `ecrecover` function is part of the EVM and is used to recover the address that signed a message. But wait, I remember that `ecrecover` can return multiple valid addresses for a single signature due to the way ECDSA signatures work. That's because of the malleability of signaturesâ€”there are multiple valid `(r, s)` pairs that can represent the same signature.\\n\\nSo, if `ecrecover` can return different addresses for the same signature, then the `verify` function might not be checking for all possible valid signatures. That could allow an attacker to forge a signature that passes the check even if it wasn't originally signed by the intended address. \\n\\nBut the function is comparing the recovered address directly to the `signer` parameter. If the signature is malleable and `ecrecover` returns a different address than the one provided, the check would fail. However, if the signature is modified in a way that still results in the same recovered address, it could be a replay of a previous signature.\\n\\nWait, the note mentions that replay attacks aren't possible here because each user's nonce is used only once. But the core issue is that `ecrecover` is malleable, and not rejecting malleable signatures is considered a best practice. So even if replay attacks aren't a concern here, the function isn't handling malleable signatures properly, which is a vulnerability.\\n</think>\\n\\nThe `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user's nonce is used only once, rejecting malleable signatures is considered a best practice.<|im_end|>\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c275ec8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:34.657025Z",
     "iopub.status.busy": "2025-06-06T18:29:34.656741Z",
     "iopub.status.idle": "2025-06-06T18:29:34.665815Z",
     "shell.execute_reply": "2025-06-06T18:29:34.664906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are the smartest AI solidity smart contract security auditor in the world<|im_end|>\\n<|im_start|>user\\nThe given Solidity function is vulnerable. Review the code and analyze its security flaws.\\n    \\n    This is the function we need to audit:\\n    ```solidity\\n    function renew(\\n        uint256 tokenId,\\n        uint256 duration,\\n        uint64 expiry\\n    ) external override onlyController returns (uint256 expires) {\\n        bytes32 node = _makeNode(ETH_NODE, bytes32(tokenId));\\n\\n        expires = registrar.renew(tokenId, duration);\\n        (address owner, uint32 fuses, uint64 oldExpiry) = getData(\\n            uint256(node)\\n        );\\n        expiry = _normaliseExpiry(expiry, oldExpiry, uint64(expires));\\n\\n        _setData(node, owner, fuses, expiry);\\n    }\\n    ```\\n    \\nVulnerability : <|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, I'm looking at this `renew` function in the contract. The function is supposed to renew a token's registration, and it's marked as `external override onlyController`, which means only the controller can call it. That makes sense for a function that's managing renewals.\\n\\nLet me start by breaking down the function step by step. The first thing it does is create a `node` using `_makeNode(ETH_NODE, bytes32(tokenId))`. This seems to be generating a unique identifier for the token, likely used to retrieve or store data related to the token.\\n\\nNext, it calls `registrar.renew(tokenId, duration)` and assigns the result to `expires`. That seems straightforward â€” it's renewing the token's registration with the registrar, which is probably the base registrar contract.\\n\\nThen, it retrieves data using `getData(uint256(node))`, which returns `owner`, `fuses`, and `oldExpiry`. This data is likely stored in some mapping or database structure, and `node` is being used as a key to fetch the current state of the token.\\n\\nThe function then calls `_normaliseExpiry(expiry, oldExpiry, uint64(expires))` to compute a new expiry time. That seems like a helper function to ensure the expiry is set correctly, perhaps taking into account the old expiry and the new duration.\\n\\nFinally, it calls `_setData(node, owner, fuses, expiry)` to update the stored data with the new expiry. That makes sense â€” after renewing the token, the expiry should be updated in the contract's state.\\n\\nNow, I need to check if there's any missing logic here. The target vulnerability mentions that the `renew` function is calling the base registrar instead of the NameWrapper, which could lead to fuses not being updated. Let me think about how fuses are handled.\\n\\nThe `getData` function returns `fuses`, which are likely some permission flags or settings for the subdomain. If the `renew` function is not using the NameWrapper, then the fuses might not be updated or passed along correctly. The `setData` function is being called, but if it's not using the NameWrapper's logic, then the fuses might not be properly updated in the context of a subdomain.\\n\\nLooking at the `renew` function again, it's calling `registrar.renew(tokenId, duration)` directly. If the `registrar` here is the base registrar and not the NameWrapper, then any subdomain-related logic, like updating fuses, would be bypassed. The NameWrapper might have additional checks or logic that needs to be executed during a renewal, especially for subdomains. If that logic isn't triggered, then the fuses for the subdomain could remain unchanged, even though the expiry has been updated.\\n\\nThis would be a problem because the fuses control permissions for the subdomain. If they aren't updated during a renewal, someone could potentially take over the subdomain if the fuses are not properly set. For example, if a fuse that prevents unauthorized renewals is not updated, an attacker might be able to renew the subdomain without proper authorization.\\n\\nSo, the key issue is that the `renew` function is not going through the NameWrapper, which is responsible for managing subdomain-related logic, including fuses. As a result, the fuses are not being updated during the renewal process, which could lead to a security vulnerability where the subdomain's permissions are not properly enforced.\\n</think>\\n\\nThe ETHRegistrarController is calling renew from base registrar and not through Namewrapper. This means the fuses for the subdomain will not be updated via _setData. This impacts the permission model set over subdomain and could lead to takeover\\n\\nProof of Concept\\n\\n1.  Observe the renew function\\n\\n    function renew(string calldata name, uint256 duration)\\n            external\\n            payable\\n            override\\n        {\\n            ...\\n\\n            uint256 expires = base.renew(uint256(label), duration);\\n\\n            ....\\n        }\\n\\n2.  As we can see this is calling renew function of Base Registrar instead of NameWrapper. Since this is not going via NameWrapper fuses will not be set\\n\\n3.  Also since renew in NameWrapper can only be called via Controller which is ETHRegistrarController so there is no way to renew subdomain<|im_end|>\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.eval_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a020d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:34.668761Z",
     "iopub.status.busy": "2025-06-06T18:29:34.668497Z",
     "iopub.status.idle": "2025-06-06T18:29:34.677849Z",
     "shell.execute_reply": "2025-06-06T18:29:34.677166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                          <think>\\nOkay, I'm looking at this `renew` function in the contract. The function is supposed to renew a token's registration, and it's marked as `external override onlyController`, which means only the controller can call it. That makes sense for a function that's managing renewals.\\n\\nLet me start by breaking down the function step by step. The first thing it does is create a `node` using `_makeNode(ETH_NODE, bytes32(tokenId))`. This seems to be generating a unique identifier for the token, likely used to retrieve or store data related to the token.\\n\\nNext, it calls `registrar.renew(tokenId, duration)` and assigns the result to `expires`. That seems straightforward â€” it's renewing the token's registration with the registrar, which is probably the base registrar contract.\\n\\nThen, it retrieves data using `getData(uint256(node))`, which returns `owner`, `fuses`, and `oldExpiry`. This data is likely stored in some mapping or database structure, and `node` is being used as a key to fetch the current state of the token.\\n\\nThe function then calls `_normaliseExpiry(expiry, oldExpiry, uint64(expires))` to compute a new expiry time. That seems like a helper function to ensure the expiry is set correctly, perhaps taking into account the old expiry and the new duration.\\n\\nFinally, it calls `_setData(node, owner, fuses, expiry)` to update the stored data with the new expiry. That makes sense â€” after renewing the token, the expiry should be updated in the contract's state.\\n\\nNow, I need to check if there's any missing logic here. The target vulnerability mentions that the `renew` function is calling the base registrar instead of the NameWrapper, which could lead to fuses not being updated. Let me think about how fuses are handled.\\n\\nThe `getData` function returns `fuses`, which are likely some permission flags or settings for the subdomain. If the `renew` function is not using the NameWrapper, then the fuses might not be updated or passed along correctly. The `setData` function is being called, but if it's not using the NameWrapper's logic, then the fuses might not be properly updated in the context of a subdomain.\\n\\nLooking at the `renew` function again, it's calling `registrar.renew(tokenId, duration)` directly. If the `registrar` here is the base registrar and not the NameWrapper, then any subdomain-related logic, like updating fuses, would be bypassed. The NameWrapper might have additional checks or logic that needs to be executed during a renewal, especially for subdomains. If that logic isn't triggered, then the fuses for the subdomain could remain unchanged, even though the expiry has been updated.\\n\\nThis would be a problem because the fuses control permissions for the subdomain. If they aren't updated during a renewal, someone could potentially take over the subdomain if the fuses are not properly set. For example, if a fuse that prevents unauthorized renewals is not updated, an attacker might be able to renew the subdomain without proper authorization.\\n\\nSo, the key issue is that the `renew` function is not going through the NameWrapper, which is responsible for managing subdomain-related logic, including fuses. As a result, the fuses are not being updated during the renewal process, which could lead to a security vulnerability where the subdomain's permissions are not properly enforced.\\n</think>\\n\\nThe ETHRegistrarController is calling renew from base registrar and not through Namewrapper. This means the fuses for the subdomain will not be updated via _setData. This impacts the permission model set over subdomain and could lead to takeover\\n\\nProof of Concept\\n\\n1.  Observe the renew function\\n\\n    function renew(string calldata name, uint256 duration)\\n            external\\n            payable\\n            override\\n        {\\n            ...\\n\\n            uint256 expires = base.renew(uint256(label), duration);\\n\\n            ....\\n        }\\n\\n2.  As we can see this is calling renew function of Base Registrar instead of NameWrapper. Since this is not going via NameWrapper fuses will not be set\\n\\n3.  Also since renew in NameWrapper can only be called via Controller which is ETHRegistrarController so there is no way to renew subdomain<|im_end|>\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.eval_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb118462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T18:29:34.680832Z",
     "iopub.status.busy": "2025-06-06T18:29:34.680569Z",
     "iopub.status.idle": "2025-06-06T22:35:38.757977Z",
     "shell.execute_reply": "2025-06-06T22:35:38.757079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,979 | Num Epochs = 5 | Total steps = 1,865\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,154,588,672 (3.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1865' max='1865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1865/1865 4:05:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.106100</td>\n",
       "      <td>1.081338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>1.081588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>1.140307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>1.258639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>1.295069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>1.372385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>1.378367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>1.454315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>1.465659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "# trainer_stats = trainer.train() << Buggy gradient accumulation\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce16e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T22:35:38.792718Z",
     "iopub.status.busy": "2025-06-06T22:35:38.792455Z",
     "iopub.status.idle": "2025-06-06T22:35:40.658817Z",
     "shell.execute_reply": "2025-06-06T22:35:40.657856Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ours_explanator\")\n",
    "tokenizer.save_pretrained(\"ours_explanator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce114b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T22:35:40.662285Z",
     "iopub.status.busy": "2025-06-06T22:35:40.661993Z",
     "iopub.status.idle": "2025-06-06T22:35:40.842072Z",
     "shell.execute_reply": "2025-06-06T22:35:40.841025Z"
    }
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"ours_explanator/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
