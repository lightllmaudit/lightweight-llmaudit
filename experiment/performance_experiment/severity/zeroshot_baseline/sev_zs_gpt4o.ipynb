{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_test = pd.read_csv(\"vuln_data_test.csv\")\n",
    "prompt = \"\"\"You need to analyze the given vulnerability explanation and classify the severity of it as \"low\", \"medium\" or \"high\" based on the impact of that vulnerability. Dont give any additional explanations just give the label (low, medium or high).\n",
    "    \n",
    "    This is the vulnerability explanation we need to analyze:\n",
    "    {explanation}\n",
    "    \n",
    "Answer: \"\"\"\n",
    "\n",
    "df_test['input_prompt'] = df_test['vuln_explanation'].map(lambda x: prompt.format(explanation=x))\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = 'You are the smartest AI solidity smart contract security auditor in the world that only answer in one word between \"low\", \"medium\" or \"high\".'\n",
    "\n",
    "# Parameters\n",
    "max_tokens_per_batch = 90000  # Set below the 900,000 token limit to be safe\n",
    "estimated_tokens_per_request = 1000  # Adjust based on your average request size\n",
    "requests_per_batch = max_tokens_per_batch // estimated_tokens_per_request\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"batch_inputs_zeroshot_class_severity\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "batch_count = 0\n",
    "request_count = 0\n",
    "batch_file = open(os.path.join(output_dir, f\"batch_input_{batch_count}.jsonl\"), \"w\")\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    user_prompt = row['input_prompt']\n",
    "\n",
    "    task = {\n",
    "        \"custom_id\": f\"request-{index}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    batch_file.write(json.dumps(task) + \"\\n\")\n",
    "    request_count += 1\n",
    "\n",
    "    if request_count >= requests_per_batch:\n",
    "        batch_file.close()\n",
    "        batch_count += 1\n",
    "        request_count = 0\n",
    "        batch_file = open(os.path.join(output_dir, f\"batch_input_{batch_count}.jsonl\"), \"w\")\n",
    "\n",
    "batch_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a86cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original CSV\n",
    "df = pd.read_csv(\"vuln_data_test_v6.csv\")\n",
    "\n",
    "# Initialize new columns\n",
    "df[\"raw_llm_out\"] = None\n",
    "\n",
    "# Path to your batch output directory\n",
    "batch_output_dir = \"batch_outputs_zeroshot_class_severity\"\n",
    "\n",
    "# Gather all output lines from multiple batch files\n",
    "output_lines = []\n",
    "for batch_idx in range(len(os.listdir(batch_output_dir))):\n",
    "    file_path = os.path.join(batch_output_dir, f\"batch_output_{batch_idx}.jsonl\")\n",
    "    with open(file_path) as f:\n",
    "        output_lines.extend(f.readlines())\n",
    "\n",
    "# Process each output line using custom_id\n",
    "for line in output_lines:\n",
    "    try:\n",
    "        json_out = json.loads(line)\n",
    "        custom_id = json_out.get(\"custom_id\", \"\")\n",
    "        string = json_out['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "        # Extract original index from custom_id like \"request-123\"\n",
    "        original_index = int(custom_id.split(\"-\")[-1])\n",
    "\n",
    "        df.loc[original_index, \"raw_llm_out\"] = string\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "        print(f\"Error processing custom_id {custom_id}: {e}\")\n",
    "        print(f\"Raw response: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0733ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df['raw_llm_out'].str.lower()\n",
    "y_true = df['severity'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36fec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high     0.5263    0.8889    0.6612        45\n",
      "         low     0.7778    0.2917    0.4242        24\n",
      "      medium     0.7241    0.5676    0.6364        74\n",
      "\n",
      "    accuracy                         0.6224       143\n",
      "   macro avg     0.6761    0.5827    0.5739       143\n",
      "weighted avg     0.6709    0.6224    0.6086       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86243de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"zeroshot_gpt4o_class_severity.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
