{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcae7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:07.652409Z",
     "iopub.status.busy": "2025-06-12T14:27:07.652196Z",
     "iopub.status.idle": "2025-06-12T14:27:07.662290Z",
     "shell.execute_reply": "2025-06-12T14:27:07.661576Z"
    }
   },
   "outputs": [],
   "source": [
    "LORA_RANK=64\n",
    "LORA_DROPOUT=0.2\n",
    "ALL_TARGET_MODULES='qkvogud'\n",
    "RSLORA=True\n",
    "MODEL_SEED=42\n",
    "EVAL_STEPS=200\n",
    "BATCH_SIZE=8\n",
    "GRADIENT_ACCUMULATION=1\n",
    "EPOCH=5\n",
    "TRAINING_SEED=3407\n",
    "WEIGHT_DECAY=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7da50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:07.665225Z",
     "iopub.status.busy": "2025-06-12T14:27:07.665013Z",
     "iopub.status.idle": "2025-06-12T14:27:31.831595Z",
     "shell.execute_reply": "2025-06-12T14:27:31.830521Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-12 14:27:16 [importing.py:53] Triton module has been replaced with a placeholder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-12 14:27:16 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 14:27:18,281\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.749 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1/2 [00:01<00:01,  1.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen3-4B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104f7e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:31.835433Z",
     "iopub.status.busy": "2025-06-12T14:27:31.834964Z",
     "iopub.status.idle": "2025-06-12T14:27:31.840203Z",
     "shell.execute_reply": "2025-06-12T14:27:31.839389Z"
    }
   },
   "outputs": [],
   "source": [
    "FONT_TO_MODULE = {\n",
    "    \"q\": \"q_proj\",\n",
    "    \"k\": \"k_proj\",\n",
    "    \"v\": \"v_proj\",\n",
    "    \"o\": \"o_proj\",\n",
    "    \"g\": \"gate_proj\",\n",
    "    \"u\": \"up_proj\",\n",
    "    \"d\": \"down_proj\"\n",
    "}\n",
    "\n",
    "# Example: ALL_TARGET_MODULES = \"gqk\"\n",
    "target_modules = [FONT_TO_MODULE[char] for char in ALL_TARGET_MODULES if char in FONT_TO_MODULE]\n",
    "\n",
    "# If you want default behavior (all modules) when empty\n",
    "if not target_modules:\n",
    "    target_modules = list(FONT_TO_MODULE.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a1e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:31.843257Z",
     "iopub.status.busy": "2025-06-12T14:27:31.843023Z",
     "iopub.status.idle": "2025-06-12T14:27:37.539810Z",
     "shell.execute_reply": "2025-06-12T14:27:37.538822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.3 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "lora_rank = LORA_RANK\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = target_modules,\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = LORA_DROPOUT, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = MODEL_SEED,\n",
    "    use_rslora = RSLORA,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "model_setup = f\"\"\"lora_rank = {LORA_RANK}\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = {target_modules},\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = {LORA_DROPOUT}, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = {MODEL_SEED},\n",
    "    use_rslora = {RSLORA},  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8032f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:37.543092Z",
     "iopub.status.busy": "2025-06-12T14:27:37.542847Z",
     "iopub.status.idle": "2025-06-12T14:27:38.338026Z",
     "shell.execute_reply": "2025-06-12T14:27:38.337161Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the paths to your dataset files\n",
    "data_files = {\n",
    "    'train': 'vuln_data_train_with_recom_CoT.csv',\n",
    "    'val': 'vuln_data_val_with_recom_CoT.csv',\n",
    "    'test': 'vuln_data_test.csv'\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cdeb94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:38.341604Z",
     "iopub.status.busy": "2025-06-12T14:27:38.341346Z",
     "iopub.status.idle": "2025-06-12T14:27:38.487492Z",
     "shell.execute_reply": "2025-06-12T14:27:38.486661Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "SYSTEM_PROMPT = [\n",
    "    \"\"\"You are the smartest AI solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the best solidity smart contract security auditor in the world\"\"\",\n",
    "    \"\"\"You are the greatest AI assistant solidity security researcher in the world\"\"\",\n",
    "    \"\"\"You are the best AI solidity smart contract security auditor in the world\"\"\"\n",
    "]\n",
    "\n",
    "PROMPTS = [\n",
    "    \"\"\"The given Solidity function is vulnerable and there was an explanation about the vulnerability. Give your patch recommendation based on that vulnerability.\n",
    "    \n",
    "    This is the vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "This is the vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"The following Solidity function contains security vulnerabilities. Based on vulnerability explanation give the recommendation to patch the security issues.\n",
    "    \n",
    "    Functions that has vulnerability:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability explanation of the function: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"The provided Solidity function has security vulnerabilities. Your task is to give recommendation to close the security issue.\n",
    "    \n",
    "    Vulnerable code:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"The Solidity function below is vulnerable. Give your best recommendation to close that security issue based on vulnerability explanation.\n",
    "    \n",
    "    Functions that vulnerable:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "Vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"Analyze the following Solidity function. It has security vulnerabilities that need to be patched. Give your best recommendation.\n",
    "    \n",
    "    Vulnerable functions:\n",
    "    ```solidity\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "The vulnerability explanation: \n",
    "{explanation}\n",
    "\n",
    "Recommendation:\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "def make_conversational(examples, reasoning_idx, system_prompt, prompt):\n",
    "    code = examples['vuln_code']\n",
    "    desc = examples['vuln_explanation']\n",
    "    recom = examples['vuln_recommendation']\n",
    "    reasoning_content = examples[f'CoT_{reasoning_idx}']\n",
    "    selected_system_prompt = system_prompt\n",
    "    selected_user_prompt = prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": selected_system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": selected_user_prompt.format(code=code, explanation=desc)},\n",
    "                    {\"role\": \"assistant\", \"reasoning_content\":reasoning_content, \"content\": recom}]\n",
    "    \n",
    "    return { \"conversations\" : conversation}\n",
    "\n",
    "# List of splits you want to process\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "# Dictionary to store concatenated datasets for each split\n",
    "concatenated_datasets = {split: [] for split in splits}\n",
    "\n",
    "# Process each dataset\n",
    "for i in range(5):\n",
    "    system_prompt = SYSTEM_PROMPT[i]\n",
    "    prompt = PROMPTS[i]\n",
    "    \n",
    "    # Create a new dataset using map\n",
    "    new_dataset = dataset.map(lambda ex: make_conversational(ex, i, system_prompt, prompt))\n",
    "    \n",
    "    # Append the correct split of the new dataset to the corresponding list\n",
    "    for split in splits:\n",
    "        concatenated_datasets[split].append(new_dataset[split])\n",
    "\n",
    "# Now concatenate all datasets for each split\n",
    "dataset = DatasetDict({\n",
    "    split: concatenate_datasets(concatenated_datasets[split]).shuffle(seed=42) for split in splits\n",
    "})\n",
    "\n",
    "dataset = dataset.remove_columns([\"vuln_title\", \"vuln_explanation\", \"severity\", \"vuln_recommendation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d965d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:38.491420Z",
     "iopub.status.busy": "2025-06-12T14:27:38.491025Z",
     "iopub.status.idle": "2025-06-12T14:27:38.875919Z",
     "shell.execute_reply": "2025-06-12T14:27:38.874953Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(\n",
    "        convo, \n",
    "        tokenize = False, \n",
    "        add_generation_prompt = False,\n",
    "        enable_thinking=False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9296bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:38.879107Z",
     "iopub.status.busy": "2025-06-12T14:27:38.878860Z",
     "iopub.status.idle": "2025-06-12T14:27:38.888044Z",
     "shell.execute_reply": "2025-06-12T14:27:38.887233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are the greatest AI assistant solidity security researcher in the world',\n",
       "  'reasoning_content': None,\n",
       "  'role': 'system'},\n",
       " {'content': 'The Solidity function below is vulnerable. Give your best recommendation to close that security issue based on vulnerability explanation.\\n    \\n    Functions that vulnerable:\\n    ```solidity\\n    function verify(\\n        address signer,\\n        MetaTransaction memory metaTx,\\n        bytes32 sigR,\\n        bytes32 sigS,\\n        uint8 sigV\\n    ) internal view returns (bool) {\\n        require(signer != address(0), \"NativeMetaTransaction: INVALID_SIGNER\");\\n        return\\n            signer ==\\n            ecrecover(\\n                toTypedMessageHash(hashMetaTransaction(metaTx)),\\n                sigV,\\n                sigR,\\n                sigS\\n            );\\n    }\\n    ```\\n    \\nVulnerability explanation: \\nThe `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user\\'s nonce is used only once, rejecting malleable signatures is considered a best practice.\\n\\nRecommendation:\\n',\n",
       "  'reasoning_content': None,\n",
       "  'role': 'user'},\n",
       " {'content': \"Recommend using the `recover` function from OpenZeppelin's ECDSA library for signature verification.\",\n",
       "  'reasoning_content': \"Okay let's see, the code is a verify function in a Solidity contract that checks if a signature is valid by using the ecrecover function. The vulnerability explanation says that ecrecover is susceptible to replay attacks because it allows malleable signatures. Even though the contract uses a nonce that's only used once, rejecting malleable signatures is still a best practice. The recommendation is to use the recover function from OpenZeppelin's ECDSA library instead. So the mitigation would involve replacing the direct call to ecrecover with the recover function from the ECDSA library, which is designed to handle these issues more securely. That should prevent replay attacks by properly rejecting malleable signatures.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bae3486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:38.890985Z",
     "iopub.status.busy": "2025-06-12T14:27:38.890744Z",
     "iopub.status.idle": "2025-06-12T14:27:38.896281Z",
     "shell.execute_reply": "2025-06-12T14:27:38.895499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are the greatest AI assistant solidity security researcher in the world<|im_end|>\\n<|im_start|>user\\nThe Solidity function below is vulnerable. Give your best recommendation to close that security issue based on vulnerability explanation.\\n    \\n    Functions that vulnerable:\\n    ```solidity\\n    function verify(\\n        address signer,\\n        MetaTransaction memory metaTx,\\n        bytes32 sigR,\\n        bytes32 sigS,\\n        uint8 sigV\\n    ) internal view returns (bool) {\\n        require(signer != address(0), \"NativeMetaTransaction: INVALID_SIGNER\");\\n        return\\n            signer ==\\n            ecrecover(\\n                toTypedMessageHash(hashMetaTransaction(metaTx)),\\n                sigV,\\n                sigR,\\n                sigS\\n            );\\n    }\\n    ```\\n    \\nVulnerability explanation: \\nThe `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user\\'s nonce is used only once, rejecting malleable signatures is considered a best practice.\\n\\nRecommendation:\\n<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay let\\'s see, the code is a verify function in a Solidity contract that checks if a signature is valid by using the ecrecover function. The vulnerability explanation says that ecrecover is susceptible to replay attacks because it allows malleable signatures. Even though the contract uses a nonce that\\'s only used once, rejecting malleable signatures is still a best practice. The recommendation is to use the recover function from OpenZeppelin\\'s ECDSA library instead. So the mitigation would involve replacing the direct call to ecrecover with the recover function from the ECDSA library, which is designed to handle these issues more securely. That should prevent replay attacks by properly rejecting malleable signatures.\\n</think>\\n\\nRecommend using the `recover` function from OpenZeppelin\\'s ECDSA library for signature verification.<|im_end|>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ab9fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:38.899240Z",
     "iopub.status.busy": "2025-06-12T14:27:38.898999Z",
     "iopub.status.idle": "2025-06-12T14:27:39.038859Z",
     "shell.execute_reply": "2025-06-12T14:27:39.037734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Max Length =  1740\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset['train'].map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"conversations\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "# print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Train Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset['train'] = dataset['train'].select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be6c395c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:39.042021Z",
     "iopub.status.busy": "2025-06-12T14:27:39.041768Z",
     "iopub.status.idle": "2025-06-12T14:27:39.184254Z",
     "shell.execute_reply": "2025-06-12T14:27:39.183309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Max Length =  1855\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset['val'].map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"conversations\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Val Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset['val'] = dataset['val'].select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38290189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:39.187500Z",
     "iopub.status.busy": "2025-06-12T14:27:39.187252Z",
     "iopub.status.idle": "2025-06-12T14:27:39.632250Z",
     "shell.execute_reply": "2025-06-12T14:27:39.631244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=EVAL_STEPS,\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size = BATCH_SIZE,\n",
    "        gradient_accumulation_steps = GRADIENT_ACCUMULATION, \n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = EPOCH,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", \n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = TRAINING_SEED,\n",
    "        load_best_model_at_end=True,\n",
    "        output_dir = \"outputs_recommendation\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "training_setup = f\"\"\"from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps={EVAL_STEPS},\n",
    "        save_steps={EVAL_STEPS},\n",
    "#         per_device_train_batch_size = 1,\n",
    "        per_device_train_batch_size = {BATCH_SIZE},\n",
    "        per_device_eval_batch_size = {BATCH_SIZE},\n",
    "        gradient_accumulation_steps = {GRADIENT_ACCUMULATION}, # Fixed major bug in latest Unsloth\n",
    "#         warmup_steps = 5,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = {EPOCH}, # Set this for 1 full training run.\n",
    "#         max_steps = 30,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Save more memory\n",
    "        weight_decay = {WEIGHT_DECAY},\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = {TRAINING_SEED},\n",
    "        load_best_model_at_end=True,\n",
    "        output_dir = \"outputs_vuln_exp_v2\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3275a622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:39.635629Z",
     "iopub.status.busy": "2025-06-12T14:27:39.635327Z",
     "iopub.status.idle": "2025-06-12T14:27:41.219410Z",
     "shell.execute_reply": "2025-06-12T14:27:41.218499Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2775098d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:41.223121Z",
     "iopub.status.busy": "2025-06-12T14:27:41.222874Z",
     "iopub.status.idle": "2025-06-12T14:27:41.230116Z",
     "shell.execute_reply": "2025-06-12T14:27:41.229360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are the greatest AI assistant solidity security researcher in the world<|im_end|>\\n<|im_start|>user\\nThe Solidity function below is vulnerable. Give your best recommendation to close that security issue based on vulnerability explanation.\\n    \\n    Functions that vulnerable:\\n    ```solidity\\n    function verify(\\n        address signer,\\n        MetaTransaction memory metaTx,\\n        bytes32 sigR,\\n        bytes32 sigS,\\n        uint8 sigV\\n    ) internal view returns (bool) {\\n        require(signer != address(0), \"NativeMetaTransaction: INVALID_SIGNER\");\\n        return\\n            signer ==\\n            ecrecover(\\n                toTypedMessageHash(hashMetaTransaction(metaTx)),\\n                sigV,\\n                sigR,\\n                sigS\\n            );\\n    }\\n    ```\\n    \\nVulnerability explanation: \\nThe `verify` function of `NativeMetaTransaction` calls the Solidity `ecrecover` function directly to verify the given signature. However, the `ecrecover` EVM opcode allows for malleable (non-unique) signatures and thus is susceptible to replay attacks. Although a replay attack on this contract is not possible since each user\\'s nonce is used only once, rejecting malleable signatures is considered a best practice.\\n\\nRecommendation:\\n<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay let\\'s see, the code is a verify function in a Solidity contract that checks if a signature is valid by using the ecrecover function. The vulnerability explanation says that ecrecover is susceptible to replay attacks because it allows malleable signatures. Even though the contract uses a nonce that\\'s only used once, rejecting malleable signatures is still a best practice. The recommendation is to use the recover function from OpenZeppelin\\'s ECDSA library instead. So the mitigation would involve replacing the direct call to ecrecover with the recover function from the ECDSA library, which is designed to handle these issues more securely. That should prevent replay attacks by properly rejecting malleable signatures.\\n</think>\\n\\nRecommend using the `recover` function from OpenZeppelin\\'s ECDSA library for signature verification.<|im_end|>\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856aa8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:41.233149Z",
     "iopub.status.busy": "2025-06-12T14:27:41.232913Z",
     "iopub.status.idle": "2025-06-12T14:27:41.240537Z",
     "shell.execute_reply": "2025-06-12T14:27:41.239749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                 <think>\\nOkay let's see, the code is a verify function in a Solidity contract that checks if a signature is valid by using the ecrecover function. The vulnerability explanation says that ecrecover is susceptible to replay attacks because it allows malleable signatures. Even though the contract uses a nonce that's only used once, rejecting malleable signatures is still a best practice. The recommendation is to use the recover function from OpenZeppelin's ECDSA library instead. So the mitigation would involve replacing the direct call to ecrecover with the recover function from the ECDSA library, which is designed to handle these issues more securely. That should prevent replay attacks by properly rejecting malleable signatures.\\n</think>\\n\\nRecommend using the `recover` function from OpenZeppelin's ECDSA library for signature verification.<|im_end|>\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c275ec8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:41.243754Z",
     "iopub.status.busy": "2025-06-12T14:27:41.243504Z",
     "iopub.status.idle": "2025-06-12T14:27:41.251392Z",
     "shell.execute_reply": "2025-06-12T14:27:41.250657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are the best AI solidity smart contract security auditor in the world<|im_end|>\\n<|im_start|>user\\nAnalyze the following Solidity function. It has security vulnerabilities that need to be patched. Give your best recommendation.\\n    \\n    Vulnerable functions:\\n    ```solidity\\n    function getLatestPrice(address num, address den) external view override returns (uint256, uint256) {\\n        uint256 _price;\\n        uint256 _decimals;\\n        (_price, _decimals) = getChainlinkLatestPrice(num, den);\\n        if (_decimals != 0) {\\n            return (_price, _decimals);\\n        }\\n        (_price, _decimals) = getUniswapLatestPrice(num, den);\\n        if (_decimals != 0) {\\n            return (_price, _decimals);\\n        }\\n        revert(\"PriceOracle::getLatestPrice - Price Feed doesn\\'t exist\");\\n    }\\n    ```\\n    \\nThe vulnerability explanation: \\nIf for whatever reason the Chainlink oracle returns a malformed price due to oracle manipulation or a malfunctioned price, the result will be passed onto users, causing unintended consequences as a result.\\n\\nIn the same time it\\'s possible to construct mitigation mechanics for such cases, so user economics be affected by sustainable price movements only. As price outrages provide a substantial attack surface for the project it\\'s worth adding some complexity to the implementation.\\n\\nThe above code outlines how prices are utilised regardless of their actual value (assuming it is always a non-zero value).\\n\\nRecommendation:\\n<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let\\'s check the functionality of the `getLatestPrice` function. It seems to be designed to fetch the latest price between two tokens using either Chainlink or Uniswap as a fallback. The function first tries Chainlink, and if that returns a non-zero `_decimals`, it returns that price. Otherwise, it tries Uniswap. If both fail, it reverts.\\n\\nHmm, this function seems suspicious in how it handles the price data from Chainlink. The code assumes that if `_decimals` is non-zero, the price is valid. But what if Chainlink returns a completely incorrect or manipulated price? The function would just pass that along without any checks.\\n\\nWait a second, if this variable isnâ€™t protected, an attacker could potentially manipulate the Chainlink oracle to return a false price. Since the function doesnâ€™t validate the price against any other source or historical data, it could propagate this incorrect value to users.\\n\\nLooking at the known vulnerability, it makes sense now. The function doesnâ€™t verify whether the Chainlink price is reasonable or consistent with other data sources. If Chainlink is compromised or returns a malformed price, the function would blindly return it, which could lead to unintended consequences like incorrect asset valuations or financial losses.\\n\\nSo, the recommendation makes sense. To mitigate this, we should compare the Chainlink price with another reliable source, like Uniswap, and ensure they are within a certain range. This would act as a safeguard against oracle manipulation. Additionally, incorporating historical price data could help detect sudden, unrealistic price changes. However, since UniswapV3\\'s TWAP is more resistant to flash loan attacks, using it as a fallback is a good approach. The main issue remains the lack of validation for Chainlink\\'s returned price, which needs to be addressed.\\n</think>\\n\\nConsider querying both the Chainlink oracle and Uniswap pool for latest prices, ensuring that these two values are within some upper/lower bounds of each other. It may also be useful to track historic values and ensure that there are no sharp changes in price. However, the first option provides a level of simplicity as UniswapV3\\'s TWAP implementation is incredibly resistant to flash loan attacks. Hence, the main issue to address is a malfunctioning Chainlink oracle.<|im_end|>\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.eval_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a020d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:41.254326Z",
     "iopub.status.busy": "2025-06-12T14:27:41.254091Z",
     "iopub.status.idle": "2025-06-12T14:27:41.262558Z",
     "shell.execute_reply": "2025-06-12T14:27:41.261841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                 <think>\\nOkay, let's check the functionality of the `getLatestPrice` function. It seems to be designed to fetch the latest price between two tokens using either Chainlink or Uniswap as a fallback. The function first tries Chainlink, and if that returns a non-zero `_decimals`, it returns that price. Otherwise, it tries Uniswap. If both fail, it reverts.\\n\\nHmm, this function seems suspicious in how it handles the price data from Chainlink. The code assumes that if `_decimals` is non-zero, the price is valid. But what if Chainlink returns a completely incorrect or manipulated price? The function would just pass that along without any checks.\\n\\nWait a second, if this variable isnâ€™t protected, an attacker could potentially manipulate the Chainlink oracle to return a false price. Since the function doesnâ€™t validate the price against any other source or historical data, it could propagate this incorrect value to users.\\n\\nLooking at the known vulnerability, it makes sense now. The function doesnâ€™t verify whether the Chainlink price is reasonable or consistent with other data sources. If Chainlink is compromised or returns a malformed price, the function would blindly return it, which could lead to unintended consequences like incorrect asset valuations or financial losses.\\n\\nSo, the recommendation makes sense. To mitigate this, we should compare the Chainlink price with another reliable source, like Uniswap, and ensure they are within a certain range. This would act as a safeguard against oracle manipulation. Additionally, incorporating historical price data could help detect sudden, unrealistic price changes. However, since UniswapV3's TWAP is more resistant to flash loan attacks, using it as a fallback is a good approach. The main issue remains the lack of validation for Chainlink's returned price, which needs to be addressed.\\n</think>\\n\\nConsider querying both the Chainlink oracle and Uniswap pool for latest prices, ensuring that these two values are within some upper/lower bounds of each other. It may also be useful to track historic values and ensure that there are no sharp changes in price. However, the first option provides a level of simplicity as UniswapV3's TWAP implementation is incredibly resistant to flash loan attacks. Hence, the main issue to address is a malfunctioning Chainlink oracle.<|im_end|>\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.eval_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb118462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T14:27:41.265677Z",
     "iopub.status.busy": "2025-06-12T14:27:41.265427Z",
     "iopub.status.idle": "2025-06-12T18:42:37.281572Z",
     "shell.execute_reply": "2025-06-12T18:42:37.280602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,979 | Num Epochs = 5 | Total steps = 1,865\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,154,588,672 (3.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1865' max='1865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1865/1865 4:14:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.696095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.701568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>0.715334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.772534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>0.785603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.874038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.879373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.938340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.939147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce16e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:42:37.318645Z",
     "iopub.status.busy": "2025-06-12T18:42:37.318404Z",
     "iopub.status.idle": "2025-06-12T18:42:39.574455Z",
     "shell.execute_reply": "2025-06-12T18:42:39.573442Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ours_recom\")  # Local saving\n",
    "tokenizer.save_pretrained(\"ours_recom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce114b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:42:39.577816Z",
     "iopub.status.busy": "2025-06-12T18:42:39.577542Z",
     "iopub.status.idle": "2025-06-12T18:42:39.761924Z",
     "shell.execute_reply": "2025-06-12T18:42:39.760823Z"
    }
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"ours_recom/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
